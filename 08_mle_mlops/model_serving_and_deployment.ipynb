{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üì¶ Model Serving & Deployment (MLOps)\n",
        "\n",
        "In this notebook, we will see how to make a previously trained machine learning model available so that other people or applications can use it.\n",
        "To do this, we will:\n",
        "\n",
        "‚úÖ Serve a model as a **REST API** using **FastAPI**\n",
        "\n",
        "‚úÖ Load a trained model and use it to make predictions through the API\n",
        "\n",
        "‚úÖ Easily test the endpoint from Python\n",
        "\n",
        "‚úÖ Understand key concepts about **model deployment in production**"
      ],
      "metadata": {
        "id": "i7qLUw9F8xhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† What does ‚Äúserving a model‚Äù mean?\n",
        "\n",
        "When you train a model in an environment like Jupyter or Google Colab, only I can use it from there. If someone else wants to use it (for example, from a mobile app or a website), that model needs to be **accessible online**.\n",
        "\n",
        "To achieve this, we **serve the model as a web API**.\n",
        "This means that:\n",
        "\n",
        "- We expose a function (for example, `predict_diabetes`) through a URL\n",
        "- Any system can send data to that URL\n",
        "- The model will respond with a prediction\n",
        "\n",
        "This is the foundation of many products that use machine learning."
      ],
      "metadata": {
        "id": "bK1K7IO68_yT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Tools we will use\n",
        "\n",
        "| Tool         | What It‚Äôs Used For?                                |\n",
        "| ------------ | --------------------------------------------------- |\n",
        "| **FastAPI**  | To create modern and easy-to-use web APIs in Python |\n",
        "| **Uvicorn**  | Web server that runs FastAPI                        |\n",
        "| **Pickle**   | To save and load trained models                     |\n",
        "| **requests** | To make HTTP calls from Python and test the API     |\n"
      ],
      "metadata": {
        "id": "_cYsaqvJ9Erw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üìà Practical case: Diabetes prediction\n",
        "\n",
        "We will work with a model that predicts whether a person has diabetes, using the classic \"Pima Indians Diabetes\" dataset.\n",
        "\n",
        "The model will be a logistic regression trained with`scikit-learn`, and we will serve it as a RESTful API using `FastAPI`."
      ],
      "metadata": {
        "id": "AENrULsY9IKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Training and saving the model"
      ],
      "metadata": {
        "id": "SGVJc2lh9PvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "# 1. Load the dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/PimaIndiansDiabetes.csv\")\n",
        "\n",
        "# 2. Split the data into features (X) and target (y)\n",
        "X = df.drop(\"Class\", axis=1)\n",
        "y = df[\"Class\"]\n",
        "\n",
        "# 3. Divide the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Train a logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Save the trained model as a .pkl file\n",
        "with open(\"diabetes_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Model saved correctly as 'diabetes_model.pkl'\")"
      ],
      "metadata": {
        "id": "pXdXk2dZ8yVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97d11c7-61e6-4ade-a5f7-4ddd5e5d66aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved correctly as 'diabetes_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Creating an API to serve the model\n",
        "\n",
        "Now we will use **FastAPI** to create a server that loads the trained model and listens for incoming requests. When someone sends data, the model will respond with a prediction.\n",
        "\n",
        "The API will include:\n",
        "\n",
        "- A basic endpoint (`/`) to verify that everything is working\n",
        "- A prediction endpoint (`/predict/`) that receives input data and returns the model's result\n"
      ],
      "metadata": {
        "id": "5LVlNLnF9VYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAwgJ9PgWLjp",
        "outputId": "cf5a66a2-3e72-400f-b67d-954f1a14583a",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.3)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# 1. Load the trained model\n",
        "with open(\"diabetes_model.pkl\", \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# 2. Create the app using FastAPI\n",
        "app = FastAPI(title=\"API to predict diabetes\")\n",
        "\n",
        "# 3. Define a class to validate the input data\n",
        "class PatientData(BaseModel):\n",
        "    pregnancies: int\n",
        "    glucose: float\n",
        "    bloodpressure: float\n",
        "    skinthickness: float\n",
        "    insulin: float\n",
        "    bmi: float\n",
        "    diabetespedigreefunction: float\n",
        "    age: int\n",
        "\n",
        "# 4. Create a base endpoint to check the connection\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"API working correctly\"}\n",
        "\n",
        "# 5. Create a prediction endpoint\n",
        "@app.post(\"/predict/\")\n",
        "def predict(data: PatientData):\n",
        "    input_data = np.array([[\n",
        "        data.pregnancies, data.glucose, data.bloodpressure,\n",
        "        data.skinthickness, data.insulin, data.bmi,\n",
        "        data.diabetespedigreefunction, data.age\n",
        "    ]])\n",
        "\n",
        "    prediction = model.predict(input_data)[0]\n",
        "    result = \"Diabetic\" if prediction == 1 else \"No diabetic\"\n",
        "\n",
        "    return {\"result\": result}"
      ],
      "metadata": {
        "id": "iqgk6vn39eTL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "# Run the Server with ngrok\n",
        "nest_asyncio.apply()  # To avoid loopback connection issues in Colab\n",
        "\n",
        "# Get the ngrok token (https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "ngrok.set_auth_token(\"2wK3AZxjnOqBaV1OkpnOxDiHqmw_rXCoWtn1t12Y5wnygjr8\")\n",
        "\n",
        "# Create a tunnel to expose the FastAPI server\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Tu API p√∫blica est√° disponible en: {public_url}\")\n",
        "\n",
        "# Start the server using uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
      ],
      "metadata": {
        "id": "_Dt11eq6XrH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b2277c-b3f1-4550-cd83-8f8c83078202"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tu API p√∫blica est√° disponible en: NgrokTunnel: \"https://421b-35-202-105-124.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1240]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2a0c:5a82:8309:4800:954:aa48:7f84:eac0:0 - \"GET / HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1240]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3: Testing the API\n",
        "\n",
        "To verify that the API is working correctly, we can write a script or use an external tool to send data as if we were an external application.\n",
        "\n",
        "A great tool for this is [POSTMAN](https://www.postman.com/).\n",
        "\n",
        "We‚Äôll use the public link provided by ngrok to interact with the API:\n",
        "\n",
        "1. Start with a GET request to the base URL to check if the connection works (we should receive the default message).\n",
        "\n",
        "2. Then, perform a POST request by adding */predict/* at the end of the URL.\n",
        "\n",
        "3. In the Body tab, choose Raw, select JSON and paste the following sample input:\n",
        "```\n",
        "{\n",
        "    \"pregnancies\": 3,\n",
        "    \"glucose\": 85,\n",
        "    \"bloodpressure\": 66,\n",
        "    \"skinthickness\": 29,\n",
        "    \"insulin\": 0,\n",
        "    \"bmi\": 26.6,\n",
        "    \"diabetespedigreefunction\": 0.351,\n",
        "    \"age\": 33\n",
        "}\n",
        "```\n",
        "4. Click SEND. The model should return whether this input corresponds to a diabetic patient or not.\n",
        "\n",
        "> ‚ö†Ô∏è The model doesn't actually require the variables shown here ‚Äì the dataset columns have different names (e.g., V1, V2, ...), which causes an error"
      ],
      "metadata": {
        "id": "jbbzq77T9lz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4: Run the API Locally\n",
        "\n",
        "To launch the FastAPI server locally, use the following command:\n",
        "```bash\n",
        "uvicorn main:app --reload\n",
        "```\n",
        "This starts the API in development mode, and you can access the endpoints at: http://127.0.0.1:8000"
      ],
      "metadata": {
        "id": "I8g7PCF59x_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5: Dockerizing the API (optional)\n",
        "\n",
        "To deploy your app anywhere (e.g., cloud platforms, servers), it's good practice to create a Docker image. This ensures the application behaves the same regardless of the environment.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```dockerfile\n",
        "FROM python:3.9\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "COPY . .\n",
        "\n",
        "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n"
      ],
      "metadata": {
        "id": "NBb1Az0H-w_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How do you deploy this?**\n",
        "\n",
        "Once your API is running and (optionally) dockerized, you can deploy it to:\n",
        "\n",
        "- **Render.com** ‚Üí Very simple for FastAPI apps\n",
        "- **Hugging Face Spaces** ‚Üí Ideal if you use `Gradio` or `Streamlit`\n",
        "- **AWS/GCP/Azure** ‚Üí Scalable enterprise solutions\n",
        "- **Railway.app** or **Fly.io** ‚Üí Great options for quick prototypes\n"
      ],
      "metadata": {
        "id": "EGHtiDqO-lHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "- We have learned how to train a model, save it, and serve it as an API using FastAPI.\n",
        "- This is a crucial step in turning an ML experiment into a real product.\n"
      ],
      "metadata": {
        "id": "8e_7hRZu_Ja9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (APPENDIX A) Next steps"
      ],
      "metadata": {
        "id": "BZ1Dwqe2__ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error Handling\n",
        "\n",
        "When building an API, you can not trust that users will always send perfect data. For example, if someone sends age=\"hello\"... everything breaks.  That is why it is importante to prevent errors.\n",
        "\n",
        "With FastAPI and Pydantic, we already get basic validation, but we can improve it.\n"
      ],
      "metadata": {
        "id": "wjnD7sDUAFyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Handle missing fields or invalid data types (e.g., non-numeric values)**"
      ],
      "metadata": {
        "id": "-DySRGdqA8kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import HTTPException\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "def predict(data: PatientData):\n",
        "    try:\n",
        "        input_data = np.array([[ ... ]])\n",
        "        prediction = model.predict(input_data)[0]\n",
        "        result = \"Diabetic\" if prediction == 1 else \"No diabetic\"\n",
        "        return {\"result\": result}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=f\"Failing in prediction: {str(e)}\")"
      ],
      "metadata": {
        "id": "MC0mQJmM_OUb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CI/CD: Continuous Integration and Deployment\n",
        "\n",
        "CI/CD stands for:\n",
        "\n",
        "- CI (Continuous Integration): Automatically testing your code when changes are made.\n",
        "- CD (Continuous Deployment): Automatically deploying your app to production whenever the repository is updated (e.g., on GitHub).\n",
        "\n"
      ],
      "metadata": {
        "id": "I0H8ZFkQAXmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: CI/CD with GitHub Actions**"
      ],
      "metadata": {
        "id": "wopHTXbYBBAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We create a file at .github/workflows/deploy.yml\n",
        "\n",
        "```\n",
        "name: Deploy API\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches:\n",
        "      - main\n",
        "\n",
        "jobs:\n",
        "  build:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Set up Python\n",
        "        uses: actions/setup-python@v4\n",
        "        with:\n",
        "          python-version: '3.9'\n",
        "      - name: Install dependencies\n",
        "        run: pip install -r requirements.txt\n",
        "      - name: Run tests\n",
        "        run: python -m unittest discover tests\n",
        "```"
      ],
      "metadata": {
        "id": "vDYQpn1sBDa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. This workflow will test our code every time we push changes to the repository.\n",
        "\n",
        "Additionally, with platforms like Render.com or Railway.app, you can connect your GitHub repo and have automatic deployment every time new code is pushed."
      ],
      "metadata": {
        "id": "O0gYBh4WBFoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting the API to a Database\n",
        "\n"
      ],
      "metadata": {
        "id": "7aKzyoacBI2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you want to save each prediction made ‚Äî who made it, when, and what data was sent. To do that, you need a database."
      ],
      "metadata": {
        "id": "kEysLpC7BNCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple example using SQLite (a lightweight local file-based database):**"
      ],
      "metadata": {
        "id": "J6ugOzzBBQNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Create the database (only the first time)\n",
        "conn = sqlite3.connect(\"predictions.db\")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create the table\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS predictions (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    age INTEGER,\n",
        "    result TEXT\n",
        ")\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "mPfx65a9BMqo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, inside the prediction endpoint, we can store the input and prediction result:"
      ],
      "metadata": {
        "id": "Fq5d649CBZ8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data\n",
        "class InputData:\n",
        "    def __init__(self, age):\n",
        "        self.age = age\n",
        "\n",
        "data = InputData(age=35)\n",
        "result = \"Diabetic\""
      ],
      "metadata": {
        "id": "274etmVslMkJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions\n",
        "conn = sqlite3.connect(\"predictions.db\")\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"INSERT INTO predictions (age, result) VALUES (?, ?)\", (data.age, result))\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "OSxpY1cxBbcw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> FastAPI integrates easily with SQLite, allowing you to define your database models, create the database, and interact with it directly from your API."
      ],
      "metadata": {
        "id": "QfARqUcH0_Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual Interface with Streamlit or Gradio\n",
        "\n",
        "This allows anyone (without coding knowledge) to test the model from a web browser."
      ],
      "metadata": {
        "id": "-57IZ8LzBwrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "with open(\"diabetes_model.pkl\", \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "def prediction(preg, gluc, pressure, skin, insulin, bmi, pedigree, age):\n",
        "    data = np.array([[preg, gluc, pressure, skin, insulin, bmi, pedigree, age]])\n",
        "    pred = model.predict(data)[0]\n",
        "    return \"Diabetic\" if pred == 1 else \"No diabetic\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=prediction,\n",
        "    inputs=[\n",
        "        gr.Number(label=\"Pregnancies\"),\n",
        "        gr.Number(label=\"Glucose\"),\n",
        "        gr.Number(label=\"Bloodpressure\"),\n",
        "        gr.Number(label=\"Skinthickness\"),\n",
        "        gr.Number(label=\"Insulin\"),\n",
        "        gr.Number(label=\"BMI\"),\n",
        "        gr.Number(label=\"Diabetespedigreefunction\"),\n",
        "        gr.Number(label=\"Age\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Diabetes prediction\"\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "4kqbo4A4B7Pi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ae88f59-3445-421a-b96e-1703e2529ca7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2509bf3587bd96108a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2509bf3587bd96108a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> You can also deploy this interface directly on Hugging Face Spaces without needing your own servers."
      ],
      "metadata": {
        "id": "xNhE3kuvB85S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visual summary\n",
        "```\n",
        "[ Trained Model ]\n",
        "        ‚Üì\n",
        "[ FastAPI API (localhost) ]\n",
        "        ‚Üì\n",
        "[ Docker / Render / Hugging Face Spaces ]\n",
        "        ‚Üì\n",
        "[ Production with CI/CD ]\n",
        "        ‚Üì\n",
        "[ Visual Interface / External Apps ]\n",
        "```"
      ],
      "metadata": {
        "id": "WZOBbISWCDJ-"
      }
    }
  ]
}