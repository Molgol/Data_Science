{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WouAa5GoBgne"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üî¨ Deep Learning: CNN, RNN y Transformers\n",
        "\n",
        "Este notebook presenta una introducci√≥n te√≥rica y pr√°ctica a tres de las arquitecturas m√°s importantes en deep learning:\n",
        "\n",
        "- **CNN (Convolutional Neural Networks)** ‚Äì im√°genes\n",
        "- **RNN (Recurrent Neural Networks)** ‚Äì series temporales y texto secuencial\n",
        "- **Transformers** ‚Äì procesamiento paralelo de secuencias, NLP moderno\n",
        "- **GNN (Graph Neural Networkd)** - relaciones de nodos y aristas\n",
        "\n",
        "Adem√°s, se incluye una introducci√≥n pr√°ctica a **TensorFlow y Keras**, las librer√≠as m√°s utilizadas para construir redes neuronales.\n"
      ],
      "metadata": {
        "id": "GrHHK37L7-Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro a Tensorflow y Keras"
      ],
      "metadata": {
        "id": "FvpDcv-T8GOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt0mFK8u76-X"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¬øQu√© es TensorFlow?\n",
        "\n",
        "TensorFlow es una plataforma open-source desarrollada por Google para construir y entrenar modelos de machine learning y deep learning. Permite usar CPU, GPU o TPU.\n",
        "\n",
        "### ¬øQu√© es Keras?\n",
        "\n",
        "Keras es una API de alto nivel integrada en TensorFlow para construir modelos de forma m√°s r√°pida, modular e intuitiva.\n"
      ],
      "metadata": {
        "id": "g0ux6V5v8LcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN - Convolutional Neural Network\n",
        "\n",
        "### ¬øQu√© es una CNN?\n",
        "\n",
        "Las redes convolucionales est√°n dise√±adas para trabajar con datos que tienen estructura espacial, como im√°genes. Aprenden **filtros** que extraen patrones como bordes, formas y texturas.\n",
        "\n",
        "üß† **Usos t√≠picos**: Clasificaci√≥n de im√°genes, detecci√≥n de objetos, reconocimiento facial.\n"
      ],
      "metadata": {
        "id": "CvBDvKiz8O5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo con el dataset de MNIST"
      ],
      "metadata": {
        "id": "ZAvPs_b78aNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Cargar datos\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocesar\n",
        "x_train = x_train[..., tf.newaxis] / 255.0\n",
        "x_test = x_test[..., tf.newaxis] / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Modelo CNN simple\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),  # Capa convolucional: 32 filtros 3x3\n",
        "    layers.MaxPooling2D((2,2)),  # Reducci√≥n de dimensionalidad (stride 2)\n",
        "    layers.Flatten(),            # Aplanar matriz 2D a vector 1D\n",
        "    layers.Dense(64, activation='relu'),  # Capa densa totalmente conectada\n",
        "    layers.Dense(10, activation='softmax')  # Salida con 10 clases y softmax (clasificaci√≥n multiclase)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "psZCrELI8RYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Conv2D extrae patrones locales (bordes, formas).\n",
        "- MaxPooling2D reduce resoluci√≥n y ayuda a evitar overfitting.\n",
        "- Flatten convierte los datos en formato vectorial para las capas densas.\n",
        "- Dense realiza la clasificaci√≥n como en una red neuronal tradicional.\n",
        "- Softmax transforma los logits a probabilidades (suma = 1)."
      ],
      "metadata": {
        "id": "-sK6vEYO_Vj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN - Recurrent Neural Network\n",
        "\n",
        "### ¬øQu√© es una RNN?\n",
        "\n",
        "Las RNN est√°n dise√±adas para procesar datos secuenciales, como texto, audio o series temporales. Mantienen una \"memoria\" interna que ayuda a relacionar datos en diferentes pasos de la secuencia.\n",
        "\n",
        "üß† **Usos t√≠picos**: Predicci√≥n de texto, traducci√≥n autom√°tica, an√°lisis de sentimientos, predicci√≥n de series temporales."
      ],
      "metadata": {
        "id": "V7NC5tck85VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo de generar texto con un RNN LSTM"
      ],
      "metadata": {
        "id": "vJBAisqo8-3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "corpus = [\n",
        "    \"I love deep learning\",\n",
        "    \"Deep learning loves me\",\n",
        "    \"I love TensorFlow\",\n",
        "    \"I love AI\"\n",
        "]\n",
        "\n",
        "# Tokenizar texto\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "X = pad_sequences(sequences, padding='post')\n",
        "\n",
        "# Modelo LSTM simple\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8), # Codificaci√≥n de palabras en vectores densos\n",
        "    layers.LSTM(16),                                                       # Capa recurrente con 16 unidades\n",
        "    layers.Dense(1, activation='sigmoid')                                  # Salida binaria (0 o 1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "H8HZffn188bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Embedding convierte tokens a vectores densos aprendibles (ideal para texto).\n",
        "- LSTM puede ‚Äúrecordar‚Äù secuencias largas mejor que una RNN simple.\n",
        "- Dense + sigmoid sirve para clasificaci√≥n binaria."
      ],
      "metadata": {
        "id": "vTJgZG3K_rqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "### ¬øQu√© es un Transformer?\n",
        "\n",
        "Los Transformers procesan secuencias en paralelo (no paso a paso como las RNN) utilizando mecanismos de **atenci√≥n** para asignar \"peso\" a las partes m√°s relevantes de la secuencia.\n",
        "\n",
        "üß† **Usos t√≠picos**: Chatbots, traducci√≥n autom√°tica, clasificaci√≥n de texto, modelos como BERT y GPT.\n"
      ],
      "metadata": {
        "id": "4wquOEmq9Wdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo de un Transformer peque√±o en Keras"
      ],
      "metadata": {
        "id": "11oWgesl9bXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capa de atenci√≥n simple (demo)\n",
        "class SimpleSelfAttention(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.q = layers.Dense(units)  # Capa para calcular queries\n",
        "        self.k = layers.Dense(units)  # Capa para calcular keys\n",
        "        self.v = layers.Dense(units)  # Capa para calcular values\n",
        "\n",
        "    def call(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attention = tf.nn.softmax(tf.matmul(q, k, transpose_b=True), axis=-1)  # Atenci√≥n: similitud entre queries y keys\n",
        "        return tf.matmul(attention, v)  # Pondera los values por atenci√≥n\n",
        "\n",
        "# Aplicaci√≥n a una secuencia\n",
        "inputs = layers.Input(shape=(10, 64))  # Secuencia de 10 pasos con 64 features\n",
        "attention_output = SimpleSelfAttention(32)(inputs)\n",
        "model = keras.Model(inputs, attention_output)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "f3ajvtwz9YZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este m√≥dulo calcula atenci√≥n entre todos los elementos de la secuencia:\n",
        "\n",
        "- Usa producto escalar entre query y key ‚Üí qu√© tan relacionados est√°n los tokens\n",
        "- Luego multiplica por value ‚Üí recoge la informaci√≥n relevante"
      ],
      "metadata": {
        "id": "bgfWEy5b_9L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN - Graph Neural Networks\n",
        "\n",
        "Las GNN permiten trabajar con datos que tienen estructura de grafo (nodos y conexiones), como redes sociales, mol√©culas, mapas, etc.\n",
        "\n",
        "Cada nodo aprende representaciones vectoriales (embeddings) **considerando sus vecinos**.\n",
        "\n",
        "üß† **Usos t√≠picos**:\n",
        "- Recomendaciones\n",
        "- Predicci√≥n molecular\n",
        "- An√°lisis de relaciones (Knowledge Graphs)\n",
        "\n",
        "Librer√≠as comunes: PyTorch Geometric, DGL (Deep Graph Library)\n"
      ],
      "metadata": {
        "id": "WouAa5GoBgne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo conceptual"
      ],
      "metadata": {
        "id": "ZdNqj8qdCNNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Geometric: modelo simple GCN\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rItvocDjB9nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparativa"
      ],
      "metadata": {
        "id": "IJ8glR_w9nkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Arquitectura | Ideal para...                    | Ventajas                          | Desventajas                  |\n",
        "|--------------|----------------------------------|-----------------------------------|------------------------------|\n",
        "| CNN          | Im√°genes, video, visi√≥n computarizada | Muy eficiente en 2D, pocos par√°metros (patrones espaciales) | No sirve para secuencias     |\n",
        "| RNN / LSTM   | Texto, audio, series temporales       | Recuerda contexto de pasos previos | M√°s lentas, dif√≠cil de entrenar |\n",
        "| Transformers | Texto largo, NLP moderno             | Procesamiento paralelo, m√°s potentes | Requiere mucho dato y c√≥mputo |\n",
        "| GNN         | Grafos, redes, mol√©culas | Aprende de estructura relacional | Requiere librer√≠as espec√≠ficas |\n"
      ],
      "metadata": {
        "id": "bpp7uVEs9l2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (ANEXO) Transfer Learning & Fine-Tuning\n",
        "\n",
        "El Transfer Learning consiste en **reutilizar un modelo preentrenado** (normalmente en un dataset grande como ImageNet) y adaptarlo a tu problema espec√≠fico.\n",
        "\n",
        "Hay dos enfoques:\n",
        "\n",
        "1. **Feature Extraction**: Congelas el modelo base y solo entrenas una nueva capa de salida.\n",
        "2. **Fine-Tuning**: Permites que algunas capas del modelo base sigan aprendiendo en tu dataset.\n"
      ],
      "metadata": {
        "id": "lrpacTdYCxOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo de Transfer Learning con MobileNetV2"
      ],
      "metadata": {
        "id": "68Cu-8IAF9hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "\n",
        "# Cargar modelo preentrenado sin la capa final\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Congelamos todas las capas inicialmente\n",
        "base_model.trainable = False\n",
        "\n",
        "# A√±adimos nuestra cabeza de clasificaci√≥n\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "aTeNd8I_DLw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning: Descongelar y reentrenar algunas capas"
      ],
      "metadata": {
        "id": "h015Sp20GCTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora permitimos entrenar las √∫ltimas capas del modelo base\n",
        "base_model.trainable = True\n",
        "\n",
        "# Podemos elegir cu√°ntas capas descongelar\n",
        "fine_tune_at = 100\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompilar (cambiar learning rate)\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Reentrenar con fine-tuning\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=5)"
      ],
      "metadata": {
        "id": "maejVfp5GE6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (ANEXO 2) Fine Tuning en otro tipo de modelos\n",
        "\n",
        "Fine-tuning (ajuste fino) significa tomar un modelo ya entrenado (o preentrenado) y ajustarlo ligeramente para un nuevo problema o conjunto de datos espec√≠fico.\n",
        "\n",
        "**¬øPor qu√© hacer Fine Tuning?**\n",
        "- Para ahorrar tiempo y c√≥mputo, ya que reutilizas pesos aprendidos.\n",
        "- Para mejorar performance, aprovechando el conocimiento previo.\n",
        "- Ideal cuando tienes poco dato propio."
      ],
      "metadata": {
        "id": "obV-9dirG5VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tuning con Deep Learning\n",
        "\n",
        "En resumen:\n",
        "\n",
        "1. Cargas un modelo preentrenado (por ejemplo, una CNN de imagen entrenada en\n",
        "ImageNet).\n",
        "2. Le quitas o adaptas la √∫ltima capa de salida a tu tarea.\n",
        "3. Conge las capas iniciales (no se entrenan).\n",
        "4. Entrenas solo la parte final.\n",
        "5. Luego ‚Äúdescongelas‚Äù algunas capas y vuelves a entrenar todo con un learning rate m√°s bajo."
      ],
      "metadata": {
        "id": "f9wuDRa4HVbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###¬†Fine-tuning en modelos de √°rboles (Random Forest, XGBoost)\n",
        "\n",
        "Puedes ajustar hiperpar√°metros bas√°ndote en un modelo ya entrenado.\n",
        "\n",
        "En XGBoost o LightGBM, puedes usar el par√°metro xgb_model=model_previo para seguir entrenando."
      ],
      "metadata": {
        "id": "pyW4H4Z2HpDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Dataset inicial\n",
        "model1 = xgb.train(params, dtrain, num_boost_round=50)\n",
        "\n",
        "# Fine-tuning con m√°s datos\n",
        "model2 = xgb.train(params, dtrain_new, num_boost_round=20, xgb_model=model1)"
      ],
      "metadata": {
        "id": "urKqJFe8G920"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning en modelos lineales (Regresi√≥n, SVM)\n",
        "\n",
        "No se hace \"fine-tuning\" como tal, pero puedes:\n",
        "\n",
        "- Usar los coeficientes de un modelo anterior como punto de partida para otro modelo (en frameworks que lo permiten).\n",
        "- Cambiar regularizaci√≥n (C en SVM, alpha en Ridge/Lasso) y volver a entrenar con datos nuevos."
      ],
      "metadata": {
        "id": "kWch6cVHHyV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning en Embeddings preentrenados\n",
        "\n",
        "Si usas TF-IDF, Word2Vec, GloVe, etc., puedes:\n",
        "\n",
        "* Usar los embeddings ya entrenados.\n",
        "* Entrenar un nuevo modelo de clasificaci√≥n sobre ellos ‚Üí eso es un tipo de transfer + fine-tuning.\n",
        "* En NLP moderno con transformers (como BERT):\n",
        "\n",
        "Usas un modelo preentrenado de Hugging Face.\n",
        "* Le a√±ades una capa de clasificaci√≥n.\n",
        "* Lo reentrenas con tu dataset (esto es fine-tuning puro)."
      ],
      "metadata": {
        "id": "pJW-kQkCH6-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusi√≥n\n",
        "\n",
        "| Tipo de modelo               | ¬øSe puede hacer fine-tuning? | ¬øC√≥mo se aplica? |\n",
        "|------------------------------|-------------------------------|------------------|\n",
        "| **CNN / RNN / Transformers** | ‚úÖ S√≠                          | Congelando capas preentrenadas, a√±adiendo nuevas capas de salida, reentrenando con learning rate bajo |\n",
        "| **XGBoost / LightGBM**       | ‚úÖ S√≠                          | Continuando el entrenamiento con nuevos datos usando `xgb_model` |\n",
        "| **Regresi√≥n / SVM**          | ‚ö†Ô∏è Limitado                   | Ajustando hiperpar√°metros, reutilizando coeficientes como punto de partida (si es posible) |\n",
        "| **Modelos NLP cl√°sicos**     | ‚úÖ S√≠                          | Usando embeddings preentrenados (TF-IDF, Word2Vec, GloVe) como entrada para nuevos clasificadores |\n",
        "| **Transformers (BERT, GPT)** | ‚úÖ S√≠ (muy com√∫n)              | A√±adiendo cabeza de clasificaci√≥n y reentrenando el modelo completo o parcialmente |\n"
      ],
      "metadata": {
        "id": "D29LKdjsIOM7"
      }
    }
  ]
}