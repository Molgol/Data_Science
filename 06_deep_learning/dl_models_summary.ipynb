{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WouAa5GoBgne"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🔬 Deep Learning Algorithms\n",
        "\n",
        "This notebook presents a theoretical and practical introduction to four of the most important architectures in deep learning:\n",
        "\n",
        "- **CNN** (Convolutional Neural Networks) – images\n",
        "- **RNN** (Recurrent Neural Networks) – time series and sequential text\n",
        "- **Transformers** – parallel sequence processing, modern NLP\n",
        "- **GNN** (Graph Neural Networks) – relationships between nodes and edges\n",
        "\n",
        "In order to do so, we will use TensorFlow and Keras, the most commonly used libraries for building neural networks."
      ],
      "metadata": {
        "id": "GrHHK37L7-Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro to Tensorflow y Keras"
      ],
      "metadata": {
        "id": "FvpDcv-T8GOx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yt0mFK8u76-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa951b8-b229-430a-a8d6-7ec51adef110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is TensorFlow?\n",
        "\n",
        "TensorFlow is an open-source platform developed by Google for **building and training** machine learning and deep learning models. It supports the use of CPU, GPU, or TPU.\n",
        "\n",
        "### What is Keras?\n",
        "\n",
        "Keras is a high-level API integrated into TensorFlow that enables building models in a faster, more modular, and intuitive way.\n"
      ],
      "metadata": {
        "id": "g0ux6V5v8LcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN - Convolutional Neural Network\n",
        "\n",
        "### What is a CNN?\n",
        "\n",
        "Convolutional Neural Networks are designed to work with data that has **spatial structure**, such as images. They learn filters that extract patterns like edges, shapes, and textures.\n",
        "\n",
        "🧠 **Typical uses**: Image classification, object detection, facial recognition.\n"
      ],
      "metadata": {
        "id": "CvBDvKiz8O5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example with MNIST dataset"
      ],
      "metadata": {
        "id": "ZAvPs_b78aNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess\n",
        "x_train = x_train[..., tf.newaxis] / 255.0\n",
        "x_test = x_test[..., tf.newaxis] / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Simple CNN\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),  # Convolutional layer: 32 filters of size 3x3\n",
        "    layers.MaxPooling2D((2,2)),  # Dimensionality reduction (stride 2)\n",
        "    layers.Flatten(),            # Flatten 2D matrix to 1D vector\n",
        "    layers.Dense(64, activation='relu'),  # Fully connected dense layer\n",
        "    layers.Dense(10, activation='softmax')  # Output with 10 classes and softmax (multiclass classification)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "psZCrELI8RYk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c608df15-6fdd-4e2d-cf4a-1f574e93fd7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m346,176\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">346,176</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m347,146\u001b[0m (1.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">347,146</span> (1.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m347,146\u001b[0m (1.32 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">347,146</span> (1.32 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Conv2D extracts local patterns (edges, shapes).\n",
        "- MaxPooling2D reduces resolution and helps prevent overfitting.\n",
        "- Flatten converts the data into vector format for dense layers.\n",
        "- Dense performs classification as in a traditional neural network.\n",
        "- Softmax transforms the logits into probabilities (sum = 1)."
      ],
      "metadata": {
        "id": "-sK6vEYO_Vj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k1pcvRwqwTm",
        "outputId": "f60a095a-aca9-400d-a06b-02bf7bab4b5d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 35ms/step - accuracy: 0.8792 - loss: 0.4299 - val_accuracy: 0.9785 - val_loss: 0.0796\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 29ms/step - accuracy: 0.9781 - loss: 0.0733 - val_accuracy: 0.9842 - val_loss: 0.0607\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.9855 - loss: 0.0478 - val_accuracy: 0.9852 - val_loss: 0.0548\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 30ms/step - accuracy: 0.9897 - loss: 0.0338 - val_accuracy: 0.9798 - val_loss: 0.0669\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 28ms/step - accuracy: 0.9909 - loss: 0.0280 - val_accuracy: 0.9877 - val_loss: 0.0475\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a4bd322b090>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"\\n✅ Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Predict on a test image\n",
        "idx = 0  # Index of the test image\n",
        "sample = x_test[idx][np.newaxis, ...] # Add batch size\n",
        "\n",
        "prediction = model.predict(sample)\n",
        "predicted_class = prediction.argmax()\n",
        "\n",
        "# Show result\n",
        "plt.imshow(x_test[idx].squeeze(), cmap='gray')\n",
        "plt.title(f\"Predicted: {predicted_class}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "duDsGCEwkrmB",
        "outputId": "f2cdd036-bcab-4cc5-e6ad-32561d9c3713"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Test accuracy: 0.9861\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN - Recurrent Neural Network\n",
        "\n",
        "### What is an RNN?\n",
        "\n",
        "RNNs are designed to process **sequential data**, such as text, audio, or time series. They maintain an internal \"memory\" that helps relate data at different steps in the sequence.\n",
        "\n",
        "🧠 **Typical uses**: Text prediction, machine translation, sentiment analysis, time series forecasting."
      ],
      "metadata": {
        "id": "V7NC5tck85VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of text generation with an LSTM RNN"
      ],
      "metadata": {
        "id": "vJBAisqo8-3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "corpus = [\n",
        "    \"I love deep learning\",\n",
        "    \"Deep learning loves me\",\n",
        "    \"I love TensorFlow\",\n",
        "    \"I love AI\"\n",
        "]\n",
        "\n",
        "# Tags\n",
        "y = np.array([1, 1, 0, 0])\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "X = pad_sequences(sequences, padding='post')\n",
        "input_length = X.shape[1]\n",
        "\n",
        "# Simple LSTM model\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=8), # Encode words into dense vectors\n",
        "    layers.LSTM(16),                                                       # Recurrent layer with 16 units\n",
        "    layers.Dense(1, activation='sigmoid')                                  # Binary output (0 or 1)\n",
        "])\n",
        "model.build(input_shape=(None, input_length))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "H8HZffn188bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "b452e107-1eb2-4423-ee7b-d213799d7a53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │            \u001b[38;5;34m72\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m1,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,689\u001b[0m (6.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,689</span> (6.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,689\u001b[0m (6.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,689</span> (6.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Embedding converts tokens into learnable dense vectors (ideal for text).\n",
        "- LSTM can “remember” long sequences better than a simple RNN.\n",
        "- Dense + sigmoid is used for binary classification."
      ],
      "metadata": {
        "id": "vTJgZG3K_rqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model.fit(X, y, epochs=20, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS1yfGHUqjtm",
        "outputId": "9d82f353-ac38-451e-eb94-0554c51a8605"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.6922\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.6919\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6916\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6912\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6908\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.6905\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.6901\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.6896\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.6892\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.6887\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6882\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6876\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6871\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6865\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6858\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.6852\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.6845\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6837\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.6830\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6821\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a4bd11e4850>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"I love AI\",           # Expected from training → 0\n",
        "    \"Deep learning rocks\", # Unseen sentence but related → likely classified as 1\n",
        "    \"I love deep\",         # Partially seen → likely classified as 1\n",
        "    \"TensorFlow is great\", # New sentence with a known word → uncertain classification\n",
        "]\n",
        "\n",
        "# Preprocess the test sentences (token convertion)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "X_test = pad_sequences(test_sequences, padding='post', maxlen=input_length)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "for sentence, prob in zip(test_sentences, predictions):\n",
        "    label = 1 if prob >= 0.5 else 0\n",
        "    print(f\"Sentence: '{sentence}' → Prob: {prob[0]:.4f} → Predicted class: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHxKDFxtnnAn",
        "outputId": "e9b080d5-75de-4f6c-d9ca-b8ef26084bbf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step\n",
            "Sentence: 'I love AI' → Prob: 0.4953 → Predicted class: 0\n",
            "Sentence: 'Deep learning rocks' → Prob: 0.5009 → Predicted class: 1\n",
            "Sentence: 'I love deep' → Prob: 0.4994 → Predicted class: 0\n",
            "Sentence: 'TensorFlow is great' → Prob: 0.4941 → Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers\n",
        "\n",
        "### What is a Transformer?\n",
        "\n",
        "Transformers process sequences in parallel (not step-by-step like RNNs) using **attention mechanisms** to assign \"weight\" to the most relevant parts of the sequence.\n",
        "\n",
        "🧠 **Typical uses**: Chatbots, machine translation, text classification, models like BERT and GPT."
      ],
      "metadata": {
        "id": "4wquOEmq9Wdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conceptual example of a small model"
      ],
      "metadata": {
        "id": "11oWgesl9bXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple attention layer\n",
        "class SimpleSelfAttention(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.q = layers.Dense(units)  # Layer to compute queries\n",
        "        self.k = layers.Dense(units)  # Layer to compute keys\n",
        "        self.v = layers.Dense(units)  # Layer to compute values\n",
        "\n",
        "    def call(self, x):\n",
        "        q = self.q(x)\n",
        "        k = self.k(x)\n",
        "        v = self.v(x)\n",
        "        attention = tf.nn.softmax(tf.matmul(q, k, transpose_b=True), axis=-1)  # Attention: similarity between queries and keys\n",
        "        return tf.matmul(attention, v)                                         # Weights the values by attention\n",
        "\n",
        "# Apply of the model to a sentence\n",
        "inputs = layers.Input(shape=(10, 64))\n",
        "attention_output = SimpleSelfAttention(32)(inputs)\n",
        "model = keras.Model(inputs, attention_output)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "f3ajvtwz9YZ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "0383afa5-84dd-47a0-f80d-0d001901d4e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_self_attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m6,240\u001b[0m │\n",
              "│ (\u001b[38;5;33mSimpleSelfAttention\u001b[0m)           │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_self_attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,240</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleSelfAttention</span>)           │                        │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,240\u001b[0m (24.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,240</span> (24.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,240\u001b[0m (24.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,240</span> (24.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model computes attention between all elements of the sequence:\n",
        "\n",
        "- Uses dot product between query and key → to determine how related the tokens are\n",
        "- Then multiplies by value → to gather the relevant information"
      ],
      "metadata": {
        "id": "bgfWEy5b_9L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GNN - Graph Neural Networks\n",
        "\n",
        "GNNs allow working with data that has **graph structure** (nodes and connections), such as social networks, molecules, maps, etc.\n",
        "\n",
        "Each node learns vector representations (embeddings) by considering its neighbors.\n",
        "\n",
        "🧠 **Typical uses**: Recommendations, molecular prediction, relationship analysis (Knowledge Graphs).\n"
      ],
      "metadata": {
        "id": "WouAa5GoBgne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conceptual example"
      ],
      "metadata": {
        "id": "ZdNqj8qdCNNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-geometric\n",
        "import torch"
      ],
      "metadata": {
        "id": "jvdIT6bh5_JJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Geometric: simple GCN model\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rItvocDjB9nM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison"
      ],
      "metadata": {
        "id": "IJ8glR_w9nkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Architecture | Ideal for...                   | Advantages                  | Disadvantages                     |\n",
        "| ------------ | ------------------------------ | ------------------------------------------------------- | --------------------------------- |\n",
        "| CNN          | Images, video, computer vision | Very efficient in 2D, few parameters (spatial patterns) | Not suitable for sequences        |\n",
        "| RNN / LSTM   | Text, audio, time series       | Remembers context from previous steps                   | Slower, harder to train           |\n",
        "| Transformers | Long text, modern NLP          | Parallel processing, more powerful                      | Requires lots of data and compute |\n",
        "| GNN          | Graphs, networks, molecules    | Learns from relational structure                        | Requires specific libraries       |\n"
      ],
      "metadata": {
        "id": "bpp7uVEs9l2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (APPENDIX A) Transfer Learning\n",
        "\n",
        "Transfer Learning consists of **reusing a pre-trained model** (usually trained on a large dataset like ImageNet) and adapting it to your specific problem.\n",
        "\n",
        "There are two approaches:\n",
        "\n",
        "1. **Feature Extraction**: Freeze the base model and only train a new output layer.\n",
        "2. **Fine-Tuning**: Allow some layers of the base model to continue learning on your dataset."
      ],
      "metadata": {
        "id": "lrpacTdYCxOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Transfer Learning with MobileNetV2 (with dummy data)"
      ],
      "metadata": {
        "id": "68Cu-8IAF9hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "\n",
        "# Dummy data\n",
        "X_train = np.random.rand(100, 224, 224, 3).astype(np.float32)\n",
        "y_train = tf.keras.utils.to_categorical(np.random.randint(0, 2, size=(100,)), num_classes=2)\n",
        "\n",
        "X_val = np.random.rand(20, 224, 224, 3).astype(np.float32)\n",
        "y_val = tf.keras.utils.to_categorical(np.random.randint(0, 2, size=(20,)), num_classes=2)\n",
        "\n",
        "# Load pre-trained model without the final layer\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Initially freeze all layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add your own classification layer\n",
        "inputs = Input(shape=(224, 224, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# First train the model for our data\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5)"
      ],
      "metadata": {
        "id": "aTeNd8I_DLw_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "2a6c6bec-c375-4b35-ceef-98f18f7082b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m2,562\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,562</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,260,546\u001b[0m (8.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,260,546</span> (8.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,562\u001b[0m (10.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,562</span> (10.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.5182 - loss: 1.0369 - val_accuracy: 0.6000 - val_loss: 0.6855\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.4963 - loss: 0.8150 - val_accuracy: 0.6500 - val_loss: 0.6606\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.4342 - loss: 0.7305 - val_accuracy: 0.4000 - val_loss: 0.8623\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.5464 - loss: 0.7519 - val_accuracy: 0.4500 - val_loss: 0.7041\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.5127 - loss: 0.6848 - val_accuracy: 0.6000 - val_loss: 0.6601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fine-Tuning: Unfreeze and retrain some layers"
      ],
      "metadata": {
        "id": "h015Sp20GCTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Allow to retrain the last layers of the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Choose how many layers to unfreeze\n",
        "fine_tune_at = 100\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile (adjust learning rate)\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Retrain with fine-tuning\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5)"
      ],
      "metadata": {
        "id": "maejVfp5GE6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3254044d-bed1-4697-d734-76f1b3e18335"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 4s/step - accuracy: 0.4773 - loss: 0.9124 - val_accuracy: 0.6500 - val_loss: 0.6665\n",
            "Epoch 2/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.4979 - loss: 0.8009 - val_accuracy: 0.6500 - val_loss: 0.6712\n",
            "Epoch 3/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.5142 - loss: 0.7202 - val_accuracy: 0.6500 - val_loss: 0.6755\n",
            "Epoch 4/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.6213 - loss: 0.5846 - val_accuracy: 0.6000 - val_loss: 0.6802\n",
            "Epoch 5/5\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step - accuracy: 0.5692 - loss: 0.5997 - val_accuracy: 0.5500 - val_loss: 0.6839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (APPENDIX B) Fine-Tuning in different types of Models\n",
        "\n",
        "Fine-tuning means taking an already trained (or pre-trained) model and slightly adjusting it for a new, specific problem or dataset.\n",
        "\n",
        "**Why do Fine-Tuning?**\n",
        "- To save time and computing resources, since you reuse learned weights.\n",
        "- To improve performance by leveraging prior knowledge.\n",
        "- Ideal when you have limited data of your own."
      ],
      "metadata": {
        "id": "obV-9dirG5VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning in Deep Learning\n",
        "\n",
        "In order:\n",
        "\n",
        "1. Load a pre-trained model (e.g., an image CNN trained on ImageNet)\n",
        "2. Remove or adapt the final output layer to match your task\n",
        "3. Freeze the initial layers (they won’t be trained)\n",
        "4. Train only the final part\n",
        "5. Then “unfreeze” some layers and retrain the whole model with a lower learning rate"
      ],
      "metadata": {
        "id": "f9wuDRa4HVbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning in Tree-Based Models (Random Forest, XGBoost)\n",
        "\n",
        "You can tune hyperparameters based on an already trained model.\n",
        "\n",
        "In XGBoost or LightGBM, you can use the `xgb_model=previous_model` to continue training.\n",
        "\n",
        "```\n",
        "import xgboost as xgb\n",
        "\n",
        "# Initial dataset\n",
        "model1 = xgb.train(params, dtrain, num_boost_round=50)\n",
        "\n",
        "# Fine-tuning with more data\n",
        "model2 = xgb.train(params, dtrain_new, num_boost_round=20, xgb_model=model1)\n",
        "```"
      ],
      "metadata": {
        "id": "pyW4H4Z2HpDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning in Linear Models (Regression, SVM)\n",
        "\n",
        "There is no true \"fine-tuning\" as in deep learning, but you can:\n",
        "\n",
        "- Use the coefficients from a previous model as a starting point for another model (in frameworks that support it).\n",
        "- Change regularization (C in SVM, alpha in Ridge/Lasso) and retrain with new data."
      ],
      "metadata": {
        "id": "kWch6cVHHyV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning in Pretrained Embeddings\n",
        "\n",
        "If you're using TF-IDF, Word2Vec, etc., you can:\n",
        "\n",
        "* Use the already trained embeddings\n",
        "* Train a new classification model on top of them → this is a type of transfer + fine-tuning\n",
        "\n",
        "In modern NLP with transformers (like BERT):\n",
        "1. Use a pretrained model from Hugging Face\n",
        "2. Add a classification layer\n",
        "3. Retrain it on your dataset (this is pure fine-tuning)"
      ],
      "metadata": {
        "id": "pJW-kQkCH6-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "| Model Type    | Is Fine-tuning Possible? | How It’s Applied?                                           |\n",
        "| ---------------------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n",
        "| **CNN / RNN / Transformers** | ✅ Yes                 | Freezing pretrained layers, adding new output layers, retraining with a low learning rate |\n",
        "| **XGBoost / LightGBM**       | ✅ Yes                 | Continuing training with new data using *xgb_model*                                       |\n",
        "| **Regression / SVM**         | ⚠️ Limited            | Tuning hyperparameters, reusing coefficients as a starting point (if supported)           |\n",
        "| **Classic NLP Models**       | ✅ Yes                 | Using pretrained embeddings (TF-IDF, Word2Vec) as input for new classifiers        |\n",
        "| **Transformers (BERT, GPT)** | ✅ Yes (very common)   | Adding a classification head and retraining the whole model or part of it                 |\n",
        "\n"
      ],
      "metadata": {
        "id": "D29LKdjsIOM7"
      }
    }
  ]
}