{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE0_ztIr53fq"
   },
   "source": [
    "# \ud83d\udcac NLP: Text Preprocessing and Embeddings\n",
    "\n",
    "In this notebook, we\u2019ll cover the fundamental techniques for **preparing text** and **converting it into a useful numerical representation** for Machine Learning and Deep Learning models. We will review:\n",
    "\n",
    "- How to clean and normalize text\n",
    "- What *tokens*, *stopwords*, *lemmatization*, etc. are\n",
    "- Different ways to represent text: Bag-of-Words, TF-IDF, and Word Embeddings\n",
    "- How to use `spaCy`, `NLTK`, `scikit-learn`, and `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6YIEMEB6Fvg"
   },
   "source": [
    "## Text Cleaning and Preprocessing\n",
    "\n",
    "### Why is it necessary to preprocess text?\n",
    "\n",
    "Text data is full of noise: punctuation, capitalization, accents, irrelevant words (*stopwords*), etc. The goal of preprocessing is to transform raw text into a form that models can understand and make the most of.\n",
    "\n",
    "Useful steps:\n",
    "- Convert everything to lowercase\n",
    "- Remove punctuation and special characters\n",
    "- Remove *stopwords*: empty words like \"the\", \"and\", \"is\", ...\n",
    "- Tokenize (split text into words)\n",
    "- Lemmatize (reduce words to their base form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_7qB-G65bok",
    "outputId": "62f33c47-0ac4-493e-86ce-b08de5fa618e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'amazing',\n",
       " 'use',\n",
       " 'many',\n",
       " 'cool',\n",
       " 'application']"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import nltk   # Natural language processing library (tokenization, stopwords, etc.)\n",
    "import spacy  # Library for tokenization, lemmatization and grammatical analysis\n",
    "import re     # Regular expressions library for cleaning text\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # Load a simple text model\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Apply tokenization and lemmatize the text using the model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Save tokens in a list\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stop_words and token.is_alpha]\n",
    "    return tokens\n",
    "\n",
    "example_text = \"Natural Language Processing (NLP) is amazing! It's used in so many cool applications.\"\n",
    "preprocess_text(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmcmDSps6Org"
   },
   "source": [
    "## Representing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eX1ru-AQ6YgW"
   },
   "source": [
    "### Bag of Words (BoW)\n",
    "\n",
    "The Bag-of-Words technique converts text into vectors by **counting how many times each word appears**. It is simple and useful for many cases, although it does not capture contextual meaning.\n",
    "\n",
    "For example:\n",
    "- \"I like coffee\"\n",
    "- \"Coffee is good\"\n",
    "\n",
    "Both sentences will have a representation based on the frequency of common words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKQnii-X6YIQ",
    "outputId": "8fa0d32b-4790-4a0d-abb4-ed7ea2ed39f0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary: ['are' 'fun' 'is' 'language' 'love' 'models' 'natural' 'powerful'\n",
      " 'processing' 'text' 'tools']\n",
      "BoW matrix:\n",
      " [[0 0 0 1 1 0 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Language models are powerful tools\",\n",
    "    \"Processing text is fun\"\n",
    "]\n",
    "\n",
    "# Create a model that learns the vocabulary from the text and obtain word fequency\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n30HfgG96gcz"
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) **improves BoW by reducing the weight of frequent words** and increasing the weight of rare but meaningful words.\n",
    "\n",
    "**TF**: calculates the frequency of words in a document. A word that appears frequently in a document will have a high TF.\n",
    "\n",
    "**IDF**: calculates the frequency of each word across all documents. A word that appears in every document provides little information, so it will have a low IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKFbtMl16lud",
    "outputId": "aa84efc3-94d0-4dd4-886d-dcdbec8599db"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary: ['are' 'fun' 'is' 'language' 'love' 'models' 'natural' 'powerful'\n",
      " 'processing' 'text' 'tools']\n",
      "TF-IDF matrix:\n",
      " [[0.         0.         0.         0.42804604 0.5628291  0.\n",
      "  0.5628291  0.         0.42804604 0.         0.        ]\n",
      " [0.46735098 0.         0.         0.35543247 0.         0.46735098\n",
      "  0.         0.46735098 0.         0.         0.46735098]\n",
      " [0.         0.52863461 0.52863461 0.         0.         0.\n",
      "  0.         0.         0.40204024 0.52863461 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Same procedure as before\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF matrix:\\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUPhQ1ue6qDb"
   },
   "source": [
    "### Word Embeddings\n",
    "\n",
    "Word embeddings are dense, continuous representations of words, trained to **capture semantic relationships**. For example, in a good embedding:\n",
    "\n",
    "vector(\"king\") - vector(\"man\") + vector(\"woman\") \u2248 vector(\"queen\")\n",
    "\n",
    "This allows models to identify patterns and relationships between words in a way that is more effective than simple frequency-based methods. We will use `gensim` to load `Word2Vec` or `GloVe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9DBOGMwzdNT",
    "outputId": "ebdf1cdb-2187-4ff6-cca3-00ef1ea7f5b0",
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWJfq5FB6uYc",
    "outputId": "abd82cd0-680c-48ff-ced2-0c5a6d55016e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "'king' vector: [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
      "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
      " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
      " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
      "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
      " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
      " -0.51042 ]\n",
      "Similarity between king and queen: 0.78390425\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download pretrained embeddings\n",
    "wv = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "print(\"'king' vector:\", wv['king'])\n",
    "\n",
    "# Measure similarity\n",
    "print(\"Similarity between king and queen:\", wv.similarity('king', 'queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8jJJTjR6ybN"
   },
   "source": [
    "## Example: represent some sentences with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ly6KvF0q63X6",
    "outputId": "24065c51-3d39-4e94-bb03-dbbbd90ab4a8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Clean text: ['deep learning revolutionize ai', 'neural network core deep learning', 'nlp enable machine understand human language']\n",
      "Vocabulary: ['ai' 'core' 'deep' 'enable' 'human' 'language' 'learning' 'machine'\n",
      " 'network' 'neural' 'nlp' 'revolutionize' 'understand']\n",
      "TF-IDF:\n",
      " [[0.5628291  0.         0.42804604 0.         0.         0.\n",
      "  0.42804604 0.         0.         0.         0.         0.5628291\n",
      "  0.        ]\n",
      " [0.         0.49047908 0.37302199 0.         0.         0.\n",
      "  0.37302199 0.         0.49047908 0.49047908 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.40824829 0.40824829 0.40824829\n",
      "  0.         0.40824829 0.         0.         0.40824829 0.\n",
      "  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Deep learning is revolutionizing AI.\",\n",
    "    \"Neural networks are the core of deep learning.\",\n",
    "    \"NLP enables machines to understand human language.\"\n",
    "]\n",
    "\n",
    "# Preprocess and vectorize with TF-IDF\n",
    "clean_texts = [\" \".join(preprocess_text(t)) for t in texts]\n",
    "X_clean = tfidf.fit_transform(clean_texts)\n",
    "\n",
    "print(\"Clean text:\", clean_texts)\n",
    "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF:\\n\", X_clean.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xnb7jeMm66xE"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "- Text preprocessing is a critical step when working with text.\n",
    "- BoW and TF-IDF are simple but useful representations.\n",
    "- Word embeddings enable capturing meaning and relationships between words, going beyond basic frequency-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkXi7vJ_6Fan"
   },
   "source": [
    "## More advanced models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqQdD5bj7M8g"
   },
   "source": [
    "So far, we have learned classic methods for representing text, such as BoW or TF-IDF. While these approaches have been foundational in text processing, they have some important limitations:\n",
    "\n",
    "* They do not capture the actual meaning of words.\n",
    "* They treat all words as independent, ignoring semantic relationships.\n",
    "* They poorly handle the context in which a word appears.\n",
    "\n",
    "To overcome these limitations, more advanced word embedding techniques were developed. These allow us to:\n",
    "- Represent words as dense vectors in a space where **semantics** is reflected in the **distance and direction** of the vectors.\n",
    "- Capture relationships between words without needing to rewrite the entire corpus.\n",
    "\n",
    "Some examples of these are: Word2Vec, FastText, and BERT (Transformers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o30VHo5i7OIG"
   },
   "source": [
    "###\u00a0 Word2Vec\n",
    "\n",
    "Word2Vec is a model created by Google that transforms **words into numeric vectors**, so that words with similar meanings have vectors that are close to each other in space. The main algorithms are:\n",
    "\n",
    "- CBOW (Continuous Bag of Words): predicts a word based on its context.\n",
    "\n",
    "- Skip-gram: predicts the context based on a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kghPGW5b7X06",
    "outputId": "e30da509-7fa5-4dbf-b70c-22b3ceb78de4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similitud entre king y queen: 0.0651624\n"
     ]
    }
   ],
   "source": [
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Data for training the model\n",
    "sentences = [\n",
    "    [\"king\", \"queen\", \"man\", \"woman\"],\n",
    "    [\"paris\", \"france\", \"berlin\", \"germany\"],\n",
    "    [\"apple\", \"orange\", \"banana\", \"fruit\"]\n",
    "]\n",
    "\n",
    "# Word2Vec model: CBOW\n",
    "model_w2v = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=0)  # CBOW\n",
    "# print(\"'King' vector:\", model_w2v.wv['king'])\n",
    "\n",
    "# Measure similarity between 'king' and 'queen'\n",
    "print(\"Similitud entre king y queen:\", model_w2v.wv.similarity('king', 'queen'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js9akf2Z-G_i"
   },
   "source": [
    "> In this case does not work good because of the small data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLXnhSSFD4G3"
   },
   "source": [
    "### FastText\n",
    "\n",
    "In this case, it represents words as a **set of character n-grams**, allowing it to understand unknown words (Out Of Vocabulary \u2014 OOV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IokPWU4EBqw",
    "outputId": "879af106-1e04-427b-8617-0877cbbda351"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similar words to 'king': [('queen', 0.0559174083173275), ('paris', 0.019648853689432144), ('france', -0.008112777955830097), ('apple', -0.03811424598097801), ('fruit', -0.05331452935934067), ('woman', -0.11958061158657074), ('banana', -0.12536485493183136), ('man', -0.17913685739040375), ('orange', -0.1807599514722824), ('berlin', -0.18634317815303802)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Data for training the model\n",
    "sentences = [\n",
    "    [\"king\", \"queen\", \"man\", \"woman\"],\n",
    "    [\"paris\", \"france\", \"berlin\", \"germany\"],\n",
    "    [\"apple\", \"orange\", \"banana\", \"fruit\"]\n",
    "]\n",
    "\n",
    "# FastText model\n",
    "model_fasttext = FastText(sentences, vector_size=50, window=3, min_count=1)\n",
    "\n",
    "# Display words similar to 'king'.\n",
    "print(\"Similar words to 'king':\", model_fasttext.wv.most_similar('king'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyT6gA2BEpUb"
   },
   "source": [
    "### BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a model that reads **context from both left to right and right to left** at the same time. It was revolutionary because it understands the meaning of a word in its full context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "1003c6fecc594fe0ba477e472fbea85c",
      "eabb4abc214447d3bab4901786022209",
      "456dd1e7e19e4f1bbc3e051a9bc9cef8",
      "505f5d8f69d64ac080cc76a37603bb33",
      "dd8c323f051b43bdb53467996462a1ab",
      "0fcde82e8c7440068d10b1412ca7b752",
      "12d4b2c41bc04104b54d08cd0bda870a",
      "de2d6deef1d946bfb4d6e0a5ef4b72c5",
      "4aee298c6f4e47338b2021ef1aa334fa",
      "b86d57163c83418f8a2f2d7ac00c8e5d",
      "944da064c01a4f6790252d3505313610",
      "642d541833534ed1bc806ce9a6032189",
      "4ac7f4ecbb04430aa66f5d0b5aba49ff",
      "d55ed3ddfad3427b9238592fc3f3ba40",
      "d53edb0ccba743fba49a8db2ac0051d6",
      "f57f76d24ef145eebb6d0e2218026f2f",
      "77dd4fe8afa94bae87e28b67ffa3668d",
      "9837a410fa6b4412b859404dc305d2d9",
      "f87cd0969e354e5e8bf79070c9ab162c",
      "6089bb74bcda4775a21fc747829b8b75",
      "4c2b6489b3be4e12beb338143e88403e",
      "7f082e3b7cfd42d89dbb623aa258b818",
      "d110b0a8fb5143ff89363940a3500395",
      "f9cb2bf1084148ba9e16a8e7f4ce0486",
      "dc34228a3ff14543a2fe2e42d27862e7",
      "a0e014d50a724449b7318643cc402520",
      "ec69468cabb4415aaaeebee6f3f6c198",
      "986ca538bdec4616aadd1f23b28af49b",
      "86bdb8e6d4264b6380001f07f53029c5",
      "504735a3be4d4a64ad1877515fe97a8c",
      "77b2a18c366d45ac9164f22ab5e4be63",
      "dab75985e5c945f6a9e33643c8436d5e",
      "a75e222f5d7f4aa0bd7aaf3d9dfee5bf",
      "4175344e0e0440b09cfaf3627fa36e71",
      "ca387c90e45a47e8aa8ea07d3ff3a0ab",
      "bd7883a0891948a5a583d30bc5d08853",
      "ec9393ab4a2a46dcbdfa721a0f7e36ba",
      "3226c8e327574c959342a8f9f6e949a6",
      "1bfc1c3ee3eb4bd09c426b2506a10102",
      "5b896f8a9be544399946240023f1b118",
      "7fe8d596f2924485a49fd389cea54e65",
      "380c8e7a59994169bcd8307cebc78723",
      "c972f97b2a90405ea80b09273bcb7944",
      "85ccae7a00ca469d8d01e1aa434a68a0",
      "14db32992e9c4c81be2658a7b192f853",
      "b9da1c9fa12e49e998ec820146a7e12c",
      "0e0c0aa74c8045c08191938648fd7f5f",
      "1558d44519f647299efbdb11660cf550",
      "8639901f07994d66991bd6f1dc66fc46",
      "8a93f72384074d0bb9c8c6e772fbe8e2",
      "a086ec7f9ba14f598260aed4bed9663a",
      "0b63c9ca4de04d8a967d8a3fa521c196",
      "288ae9c941f94d5b8b2548b49c9b83c8",
      "d97fb342a3ee47f6a3de74e77a41eb93",
      "3292b318db174e2c9fad4cbea98893ad"
     ]
    },
    "id": "pmcwGKPhE3oJ",
    "outputId": "6f463a8c-ff20-4efd-a450-cc70ab6cb699"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1003c6fecc594fe0ba477e472fbea85c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "642d541833534ed1bc806ce9a6032189"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d110b0a8fb5143ff89363940a3500395"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4175344e0e0440b09cfaf3627fa36e71"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14db32992e9c4c81be2658a7b192f853"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similarity between 'bank' in different contexts: 0.4849\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the base BERT model and its tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_token_embedding(sentence, token):\n",
    "    # Tokenize the sentence and get its representations\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the index of the token\n",
    "    token_id = inputs['input_ids'][0].tolist().index(tokenizer.convert_tokens_to_ids(token))\n",
    "    # Return its embedding\n",
    "    return outputs.last_hidden_state[0][token_id].detach().numpy()\n",
    "\n",
    "# Two sentences with \"bank\" in different contexts\n",
    "sentence1 = \"He deposited money in the bank.\"\n",
    "sentence2 = \"The river overflowed the bank after the storm.\"\n",
    "\n",
    "# Get both embeddings\n",
    "embedding1 = get_token_embedding(sentence1, 'bank')\n",
    "embedding2 = get_token_embedding(sentence2, 'bank')\n",
    "\n",
    "# Calculate cosine similarity between the two embedding vectors\n",
    "cosine_similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "print(f\"Similarity between 'bank' in different contexts: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDgPe9_57Shq"
   },
   "source": [
    "## (APPENDIX A) Intro a LLMs y Chatbots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_X6Xo-Bv7nMZ"
   },
   "source": [
    "\n",
    "### What is an LLM?\n",
    "\n",
    "A **Large Language Model (LLM)** is a language model trained on massive amounts of text to predict and generate human language in a coherent way. Some well-known examples include:\n",
    "\n",
    "- GPT (OpenAI)\n",
    "- LLaMA (Meta)\n",
    "- Claude (Anthropic)\n",
    "- Mistral\n",
    "- Falcon\n",
    "- PaLM (Google)\n",
    "\n",
    "These models are capable of:\n",
    "\n",
    "\u2705 Answering questions\n",
    "\n",
    "\u2705 Translating text\n",
    "\n",
    "\u2705 Generating code\n",
    "\n",
    "\u2705 Summarizing documents\n",
    "\n",
    "\u2705 Holding conversations (like a chatbot)\n",
    "\n",
    "LLMs are based on Transformers, an architecture introduced by Vaswani et al. in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puyvrWPD7bi3"
   },
   "source": [
    "### What is a Chatbot?\n",
    "\n",
    "A chatbot is an application that simulates conversation with humans. There are two main types:\n",
    "\n",
    "- **Rule-based**: Respond using predefined patterns (if...else, decision trees)\n",
    "- **LLM-based**: Use models like GPT to generate more dynamic and natural responses\n",
    "\n",
    "Modern chatbots combine:\n",
    "- An interface (web, mobile, WhatsApp, etc.)\n",
    "- A backend with an LLM\n",
    "- Possibly a database or memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzKxwRY875lK"
   },
   "source": [
    "### DEMO: Example Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install openai"
   ],
   "metadata": {
    "id": "GHgtaXxPd5fs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "a4d69c8e-ebd2-4125-98f6-cb74bf02ae2b"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.86.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ponbas-278Ip"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# API management: https://platform.openai.com/settings/organization/api-keys\n",
    "key = \"YOUR_API_KEY_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GKEmsFtL71K",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=key)\n",
    "respuesta = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # o \"gpt-4o\"\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How does DNA work?\"}]\n",
    ")\n",
    "print(respuesta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Although the model is complete, a paid ChatGPT account is required in order to run it and call the API. However, we can use a pre-trained model using Huggingface."
   ],
   "metadata": {
    "id": "4bePLbh8eBDS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers huggingface_hub"
   ],
   "metadata": {
    "id": "rg04F49vdSJI"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356,
     "referenced_widgets": [
      "9b1325df6a8e4f0da840934f668ca29d",
      "ac918f00a55c4f569fee86c150cbf4be",
      "7fa279d489d54450b78267100eb41595",
      "06c29b2810df4a34884310ddf406a20d",
      "41fb81e5c40b42679af0ceb20655fccb",
      "533cb06f1ffe46a882f551e64131fb49",
      "6f7ee5d66b6f4c3c95b28b9819fe1d85",
      "5c58203f6fa24713a6372bbb30888f32",
      "a8d1f82a97784cada6192994572e09a6",
      "143b708171d64ca99afd10b97cb9483a",
      "12ac06c329f54ed9855f92ee8d1cf013",
      "cce7a58ede1044ada8855362c28f79ae",
      "abd01ed8800c4ce38baa1aec0f3d8308",
      "987bc9ab0bf74afa9923b1462270cbf1",
      "d8dfebad92b042fe9d82563ea1e287f0",
      "0e3c0eef000f4dbf9b193f2ce84ccce2",
      "a1fc79677ef94e8387882bde1a4fd4b6",
      "bf62cf5ff5a641efa7940af4017b965f",
      "b5f7db380c7d4374b00b24554b4ea2a1",
      "12b193f6319e424fb1f0a3ea82961e5d",
      "b6b2a85c8ce940f6b81bde8d797a6700",
      "5e1a7ab71fc54e26ac8cccd66b95d3ab",
      "b864d2a3bfe24ccc97f5c08a7a1fb6ba",
      "509915e529c54062b70fdae8d4f4ef3c",
      "c8f70f8509cb4ae19fdc71dbf8f2cb0e",
      "e4b85afbbf974fbd8756fb56051453e8",
      "a6a5861abd174f85a4ba03a175818679",
      "40eb4a000a764abf9eabb8cbb795b3ad",
      "e8ec5e1b44864762a5cc9eb77c77252c",
      "136e83f6a5d14686970d334a9b4a373d",
      "db1ced0b7d324de68fe09109779f89ae",
      "892d68a27f9748ea80a467a10ed13c2f",
      "d8c93c5e40aa4b08b1fcd8526bcaad41",
      "d792bf85ed4345f2b98595adf2f318cb",
      "cc1c92c165dc4d999c16c7c2e4e3c582",
      "8260dc608ae3403d8343a2dff789e14f",
      "2bea402ee4df470e9c10a22c8b22ae18",
      "8ee4a6eb1b2e4537aff5139346c2f7ed",
      "af06486fe40741848e71f8843976d536",
      "09c78bfc052e421bbcecfa8705b31622",
      "046daade9460416ba65db8463985038d",
      "1219bae6681f454ca9934784ca98390e",
      "0e8c48e7b9204ea495aaa1f300272305",
      "43387dd89e7e4576b43432c8c8a75b4d",
      "76c9a62bf8cc49d8bece7bcf3ccdda6e",
      "21bf2f2744cc4606b2a772b0634b6045",
      "8317f053265f481b8f62485eb87fec51",
      "d50451adeb764c44aef3529184527034",
      "1f81497c0500479989d4cee0cf74a763",
      "66f6c5c5265649bf9aaaf72779e9cb28",
      "7d4e257b7dd14a34ab6be75003a071be",
      "5cf3cc9642d44e2db0f86b7e34c0d8cd",
      "6e4bcd04fb2f4556bc0eec0a1f9a96b1",
      "54b088c485b14c468143e0c38427bcbb",
      "916b4401e04c455a8716d17dc69800ca",
      "efd3d2df65624246abd471d22134252a",
      "e6a5477ec658489bb8e8d2f875b81d21",
      "1172240129734893bfc22ab648bcef1f",
      "c6e25fd59551493da945fd234220de50",
      "7b3a54c7dbef48e8a9234f3935446ae9",
      "ca13e08fe13949bc8bf13ce9c594b7e7",
      "160706b9f3f94d058e541cb275e39647",
      "d76a658617654eb5a558903314995822",
      "6c5657f3e1bc400facac1ee326aa65d6",
      "4022bf8a1e5a41799fb68c0a10d5d54c",
      "08db38397a9441379c971c7b1934b432",
      "e8f99841832c43caaf8cef744a09d87f",
      "148c9c458c5a49d29fecb42efc249f4e",
      "33b9283b1dfc4b44a256d13f2f2d23b1",
      "64db87ef61a1401caa3d7c040ead1c9c",
      "90239560980e49458e8929bd6d326bf0",
      "40d37f032ad44e3795897959f8cbd822",
      "72b09cfecdd6432a8bac493eb7a1a9e6",
      "7a28f3ef3f3e4797a9aa3060d586bd7d",
      "727c680869184f3291f4dbc323ec353c",
      "13111a7cadef4007b68e749ac1c0605f",
      "5f06cfc0c0c54252bce058233e0bd1b0",
      "7a3e01cb71a64ae2a0c790907aeaa779",
      "6973bdafcabf450d90c78e3aaf828692",
      "faafeceb59f044aa8c88ad6075d2cd0a",
      "c5c5d67715514c159f99f46574eb0c93",
      "c53f14df2950401fac9d8f80d8694e55",
      "bae243321ed24873827b94230040a808",
      "82bcec1ab8db4f67950c814aa520ae08",
      "cf0c125448a14dc4bc0d9c057206a268",
      "4f30b24a6f944ae19d0a871de427a60c",
      "73495485178f4ad2bd20fcc13901dde3",
      "78743eb4b2c940e7bee45dab4b80cd06",
      "190dc99eb9c54d4a8eab86482066772c",
      "db38784597b44ee7a2ccd222ddc1bb4a",
      "d893ec1132754808976b91c0c01312bd",
      "998add03ccd24990b50bc7ae526fda1e",
      "92938af771fe4c57b9ad03a0a077068f",
      "74935dfbc4594307970609d98addb0a0",
      "7290e2900ebf40508cfc5031d4189eb3",
      "005bbe19cd3c4f0295424d50d2b5fa7b",
      "e01c2e67436b4f3a881e848399680e6c",
      "8bb51f06b4d44348b5df72bfcad61634",
      "9a8835643d8543b79e031bcf999c97d2"
     ]
    },
    "id": "j59tVTtVMqev",
    "outputId": "2a3239ad-3e94-435c-b425-460caad11161"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b1325df6a8e4f0da840934f668ca29d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cce7a58ede1044ada8855362c28f79ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b864d2a3bfe24ccc97f5c08a7a1fb6ba"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d792bf85ed4345f2b98595adf2f318cb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76c9a62bf8cc49d8bece7bcf3ccdda6e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efd3d2df65624246abd471d22134252a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8f99841832c43caaf8cef744a09d87f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a3e01cb71a64ae2a0c790907aeaa779"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "190dc99eb9c54d4a8eab86482066772c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Why is important to drink water?\n",
      "Water is important for the body to function properly. It is the main component of the body.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Configure the model\n",
    "model_id = \"tiiuae/falcon-rw-1b\"\n",
    "token = \"YOUR_TOKEN_HERE\"  # Token management in https://huggingface.co/settings/tokens\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", token=token)\n",
    "\n",
    "def chatbot(pregunta):\n",
    "    inputs = tokenizer(pregunta, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example phares\n",
    "# print(chatbot(\"How does photosynthesis work?\"))\n",
    "print(chatbot(\"Why is important to drink water?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCoklqvI9XO1"
   },
   "source": [
    "## (APPENDIX B) LLMs integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZGDm7TS-Xsz"
   },
   "source": [
    "### FastAPI: Fast and modern API with Python\n",
    "\n",
    "\ud83d\udd0d What is it?\n",
    "\n",
    "FastAPI is a web framework for **building fast, secure, and easily scalable APIs**. It is based on standard Python and is ideal for serving ML models or connecting a frontend with an intelligent backend (e.g., a chatbot).\n",
    "\n",
    "\ud83d\ude80 Why use it?\n",
    "\n",
    "- Super fast (uses uvicorn and async)\n",
    "- Ideal for microservices\n",
    "- Automatically generates Swagger documentation\n",
    "- Compatible with ML frameworks (scikit-learn, PyTorch, TensorFlow, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install fastapi"
   ],
   "metadata": {
    "id": "GQZjvfdrdOOw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "outputId": "d0f6e851-cd0c-4442-d18d-698f41383247"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HY3G4aMe9Zb7"
   },
   "outputs": [],
   "source": [
    "# Example: API that responds using ChatGPT\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "\n",
    "# Create an instance\n",
    "app = FastAPI()\n",
    "openai.api_key = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# The model receives a string message sent by the user\n",
    "class Prompt(BaseModel):\n",
    "    message: str\n",
    "\n",
    "# Define a POST endpoint at the \"/chat/\" route\n",
    "@app.post(\"/chat/\")\n",
    "\n",
    "# This endpoint receives a JSON with the message\n",
    "def chat(prompt: Prompt):\n",
    "    # Call the OpenAI API to generate a chat response\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt.message}]\n",
    "    )\n",
    "    # And return the response\n",
    "    return {\"reply\": response['choices'][0]['message']['content']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbW2i5JI-m5O"
   },
   "source": [
    "> To test locally: uvicorn app:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jqlQKev-p2f"
   },
   "source": [
    "###\u00a0LangChain: LLMs with tools, memory, and logic\n",
    "\n",
    "LangChain is a library for **building applications with LLMs** that can:\n",
    "- Retain conversation memory\n",
    "- Access documents, APIs, or databases\n",
    "- Integrate with intelligent agents and external tools\n",
    "\n",
    "\ud83c\udfaf Ideal for:\n",
    "- Contextual chatbots with memory\n",
    "- QA systems over PDFs\n",
    "- Autonomous agents with logic and decision-making\n",
    "\n",
    "\ud83e\udde0 Key Concepts:\n",
    "* Chains: Sequences of steps (input \u2192 prompt \u2192 LLM \u2192 output)\n",
    "* Memory: Stores the conversation history\n",
    "* Tools: Access to external sources (Google, Python, databases, etc.)\n",
    "* Agents: Models that decide which tool to use at each step"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}