{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Q6YIEMEB6Fvg",
        "QmcmDSps6Org",
        "h8jJJTjR6ybN",
        "wDgPe9_57Shq",
        "vCoklqvI9XO1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üí¨ NLP: Preprocesamiento de Texto y Embeddings\n",
        "\n",
        "En este notebook exploraremos las t√©cnicas fundamentales para preparar texto y convertirlo en una representaci√≥n num√©rica √∫til para modelos de Machine Learning y Deep Learning. Aprender√°s:\n",
        "\n",
        "- C√≥mo limpiar y normalizar texto\n",
        "- Qu√© son los *tokens*, *stopwords*, *lemmatization*, etc.\n",
        "- Diferentes formas de representar texto: Bag-of-Words, TF-IDF y Word Embeddings\n",
        "- C√≥mo utilizar `spaCy`, `NLTK`, `scikit-learn`, y `gensim`\n",
        "\n",
        "Combinaremos teor√≠a con ejercicios pr√°cticos para que puedas aplicar todo en tus propios proyectos."
      ],
      "metadata": {
        "id": "qE0_ztIr53fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpieza y preprocesamiento de texto\n",
        "\n",
        "### ¬øPor qu√© es necesario preprocesar el texto?\n",
        "\n",
        "Los datos textuales est√°n llenos de ruido: puntuaci√≥n, may√∫sculas, acentos, palabras irrelevantes (*stopwords*), etc. El objetivo del preprocesamiento es transformar el texto crudo en una forma que los modelos puedan entender y aprovechar al m√°ximo.\n",
        "\n",
        "Pasos comunes:\n",
        "- Pasar todo a min√∫sculas\n",
        "- Eliminar puntuaci√≥n y caracteres especiales\n",
        "- Eliminar *stopwords*\n",
        "- Tokenizar (dividir el texto en palabras)\n",
        "- Lematizar (reducir palabras a su forma base)\n"
      ],
      "metadata": {
        "id": "Q6YIEMEB6Fvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_7qB-G65bok"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Min√∫sculas\n",
        "    text = text.lower()\n",
        "    # Eliminar puntuaci√≥n y n√∫meros\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenizaci√≥n y lematizaci√≥n\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if token.text not in stop_words and token.is_alpha]\n",
        "    return tokens\n",
        "\n",
        "example_text = \"Natural Language Processing (NLP) is amazing! It's used in so many cool applications.\"\n",
        "preprocess_text(example_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Representaciones de texto"
      ],
      "metadata": {
        "id": "QmcmDSps6Org"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words (BoW)\n",
        "\n",
        "La t√©cnica de Bag-of-Words convierte texto en vectores contando cu√°ntas veces aparece cada palabra. Es sencilla y √∫til para muchos casos, aunque no captura el significado contextual.\n",
        "\n",
        "Por ejemplo:\n",
        "- \"me gusta el caf√©\"\n",
        "- \"el caf√© es bueno\"\n",
        "\n",
        "Ambas frases tendr√°n una representaci√≥n basada en la frecuencia de palabras comunes.\n"
      ],
      "metadata": {
        "id": "eX1ru-AQ6YgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Language models are powerful tools\",\n",
        "    \"Processing text is fun\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulario:\", vectorizer.get_feature_names_out())\n",
        "print(\"Matriz BoW:\\n\", X.toarray())"
      ],
      "metadata": {
        "id": "FKQnii-X6YIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "TF-IDF (Term Frequency - Inverse Document Frequency) mejora BoW al reducir el peso de palabras frecuentes y aumentar el peso de palabras raras pero significativas."
      ],
      "metadata": {
        "id": "n30HfgG96gcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulario:\", tfidf.get_feature_names_out())\n",
        "print(\"Matriz TF-IDF:\\n\", X_tfidf.toarray())"
      ],
      "metadata": {
        "id": "jKFbtMl16lud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings\n",
        "\n",
        "Las *word embeddings* son representaciones densas y continuas de palabras, entrenadas para capturar relaciones sem√°nticas. Por ejemplo, en un buen embedding:\n",
        "\n",
        "- vector(\"rey\") - vector(\"hombre\") + vector(\"mujer\") ‚âà vector(\"reina\")\n",
        "\n",
        "Usaremos `gensim` para cargar `Word2Vec` o `glove`.\n"
      ],
      "metadata": {
        "id": "zUPhQ1ue6qDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Descargar embeddings preentrenados\n",
        "wv = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "print(\"Vector de 'king':\", wv['king'])\n",
        "\n",
        "# Medir similitud\n",
        "print(\"Similitud entre king y queen:\", wv.similarity('king', 'queen'))"
      ],
      "metadata": {
        "id": "oWJfq5FB6uYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio: representar un conjunto de datos"
      ],
      "metadata": {
        "id": "h8jJJTjR6ybN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "    \"Deep learning is revolutionizing AI.\",\n",
        "    \"Neural networks are the core of deep learning.\",\n",
        "    \"NLP enables machines to understand human language.\"\n",
        "]\n",
        "\n",
        "# Preprocesar y vectorizar con TF-IDF\n",
        "clean_texts = [\" \".join(preprocess_text(t)) for t in texts]\n",
        "X_clean = tfidf.fit_transform(clean_texts)\n",
        "\n",
        "print(\"Texto limpio:\", clean_texts)\n",
        "print(\"TF-IDF:\\n\", X_clean.toarray())"
      ],
      "metadata": {
        "id": "Ly6KvF0q63X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones\n",
        "\n",
        "- El preprocesamiento textual es un paso cr√≠tico para trabajar con texto.\n",
        "- BoW y TF-IDF son representaciones sencillas pero √∫tiles.\n",
        "- Los *word embeddings* permiten capturar significado y relaciones entre palabras.\n",
        "- En pr√≥ximos notebooks exploraremos modelos como Word2Vec, FastText y Transformers como BERT."
      ],
      "metadata": {
        "id": "Xnb7jeMm66xE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (ANEXO) Intro a LLMs y Chatbots\n"
      ],
      "metadata": {
        "id": "wDgPe9_57Shq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ¬øQu√© es un LLM?\n",
        "\n",
        "Un **Large Language Model (LLM)** es un modelo de lenguaje entrenado con enormes cantidades de texto para predecir y generar lenguaje humano de forma coherente. Algunos ejemplos conocidos:\n",
        "\n",
        "- GPT (OpenAI)\n",
        "- LLaMA (Meta)\n",
        "- Claude (Anthropic)\n",
        "- Mistral\n",
        "- Falcon\n",
        "- PaLM (Google)\n",
        "\n",
        "Estos modelos son capaces de:\n",
        "\n",
        "‚úÖ Responder preguntas  \n",
        "‚úÖ Traducir texto  \n",
        "‚úÖ Generar c√≥digo  \n",
        "‚úÖ Resumir documentos  \n",
        "‚úÖ Mantener conversaciones (como un chatbot)\n",
        "\n",
        "Los LLM se basan en **Transformers**, una arquitectura introducida por Vaswani et al. en 2017."
      ],
      "metadata": {
        "id": "_X6Xo-Bv7nMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¬øQu√© es un Chatbot?\n",
        "\n",
        "Un chatbot es una aplicaci√≥n que simula conversaci√≥n con humanos. Hay dos grandes tipos:\n",
        "\n",
        "- **Reglas fijas**: Responden con patrones predefinidos (if...else, √°rboles de decisi√≥n)\n",
        "- **Basados en LLMs**: Usan modelos como GPT para generar respuestas din√°micas y m√°s naturales\n",
        "\n",
        "Los chatbots modernos combinan:\n",
        "- Una interfaz (web, m√≥vil, WhatsApp, etc.)\n",
        "- Un backend con un LLM\n",
        "- Posiblemente una base de datos o memoria"
      ],
      "metadata": {
        "id": "puyvrWPD7bi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEMO: Chatbot de ejemplo"
      ],
      "metadata": {
        "id": "nzKxwRY875lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necesitas instalar primero openai si no lo tienes:\n",
        "# !pip install openai\n",
        "\n",
        "import openai\n",
        "\n",
        "# Gesti√≥n de API: https://platform.openai.com/settings/organization/api-keys\n",
        "openai.api_key = \"TU_API_KEY\"  # Usa tu clave de OpenAI o carga desde entorno\n",
        "\n",
        "def ask_gpt(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # O gpt-4 si tienes acceso\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "ask_gpt(\"¬øCu√°l es la diferencia entre una red neuronal y un √°rbol de decisi√≥n?\")"
      ],
      "metadata": {
        "id": "ponbas-278Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### FROM OpenAI.com\n",
        "#from openai import OpenAI\n",
        "#client = OpenAI()\n",
        "#\n",
        "#response = client.responses.create(\n",
        "#    model=\"gpt-4.1\",\n",
        "#    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        "#)\n",
        "#\n",
        "#print(response.output_text)"
      ],
      "metadata": {
        "id": "Y1L6peB58Ude"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### App Web de Chatbot con Streamlit"
      ],
      "metadata": {
        "id": "SNgWFM5k9C-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala streamlit si no lo tienes:\n",
        "# !pip install streamlit openai\n",
        "\n",
        "import streamlit as st\n",
        "import openai\n",
        "\n",
        "openai.api_key = st.secrets[\"OPENAI_API_KEY\"]  # O ponla directo (no recomendado)\n",
        "\n",
        "st.title(\"ü§ñ Chatbot con GPT\")\n",
        "\n",
        "user_input = st.text_input(\"Escribe tu mensaje\")\n",
        "\n",
        "if user_input:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": user_input}]\n",
        "    )\n",
        "    st.write(response['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "tvLPOIr-9FqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamlit run chatbot_app.py"
      ],
      "metadata": {
        "id": "nMsgL2fy9IAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (ANEXO 2) Applicaci√≥n Chatbot con memor√≠a\n",
        "\n",
        "Empezamos con un poco de teor√≠a"
      ],
      "metadata": {
        "id": "vCoklqvI9XO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FastAPI: API r√°pida y moderna con Python\n",
        "\n",
        "üîç ¬øQu√© es?\n",
        "\n",
        "FastAPI es un framework para crear APIs web r√°pidas, seguras y f√°ciles de escalar. Se basa en Python est√°ndar (usando type hints) y es ideal para servir modelos de ML o conectar un frontend con un backend inteligente (por ejemplo, un chatbot).\n",
        "\n",
        "üöÄ ¬øPor qu√© usarlo?\n",
        "- Super r√°pido (usa uvicorn y async)\n",
        "- Ideal para microservicios\n",
        "- Genera autom√°ticamente documentaci√≥n Swagger\n",
        "- Compatible con frameworks ML (scikit, PyTorch, TensorFlow, etc.)"
      ],
      "metadata": {
        "id": "IZGDm7TS-Xsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: API que responde con ChatGPT\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import openai\n",
        "\n",
        "app = FastAPI()\n",
        "openai.api_key = \"TU_API_KEY\"\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    message: str\n",
        "\n",
        "@app.post(\"/chat/\")\n",
        "def chat(prompt: Prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt.message}]\n",
        "    )\n",
        "    return {\"reply\": response['choices'][0]['message']['content']}"
      ],
      "metadata": {
        "id": "HY3G4aMe9Zb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Para probarla: uvicorn app:app --reload"
      ],
      "metadata": {
        "id": "kbW2i5JI-m5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###¬†LangChain: LLMs con herramientas, memoria y l√≥gica\n",
        "\n",
        "üîç ¬øQu√© es?\n",
        "\n",
        "LangChain es una librer√≠a para construir aplicaciones con LLMs que pueden:\n",
        "- Tener memoria de la conversaci√≥n\n",
        "- Acceder a documentos, APIs o bases de datos\n",
        "- Integrarse con agentes inteligentes y herramientas externas\n",
        "\n",
        "üéØ Ideal para:\n",
        "- Chatbots contextuales con memoria\n",
        "- Sistemas de QA sobre PDFs\n",
        "- Agentes aut√≥nomos con l√≥gica y decisiones\n",
        "\n",
        "üß† Conceptos clave:\n",
        "* Chains: flujos de pasos (input ‚Üí prompt ‚Üí LLM ‚Üí output)\n",
        "* Memory: guarda el historial de la conversaci√≥n\n",
        "* Tools: acceso a fuentes externas (Google, Python, DBs, etc.)\n",
        "* Agents: modelos que deciden qu√© herramienta usar en cada momento"
      ],
      "metadata": {
        "id": "3jqlQKev-p2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: Chatbot con memoria\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chat = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "chat.run(\"Hola, ¬øqui√©n eres?\")\n",
        "chat.run(\"¬øY qu√© puedes hacer?\")"
      ],
      "metadata": {
        "id": "49_BLS_n-o8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proyecto LLM\n",
        "\n",
        "Un chatbot con memoria, orquestado con LangChain, que utiliza un modelo open-source de HuggingFace, y es expuesto v√≠a FastAPI como una API que puede usarse desde cualquier frontend (web, app, etc)."
      ],
      "metadata": {
        "id": "1lpADCnK_Iez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DARLE UNA VUELTA PARA HACER QUE SEA UN PROYECTO\n",
        "\n",
        "1. requirements.txt\n",
        "\n",
        "fastapi\\\n",
        "uvicorn\\\n",
        "langchain\\\n",
        "transformers\\\n",
        "torch"
      ],
      "metadata": {
        "id": "G3q0KQzv_kxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chatbot.py : L√≥gica del chatbot\n",
        "\n",
        "#####\n",
        "# AutoModelForCausalLM: Cargamos un modelo de lenguaje causal (tipo GPT)\n",
        "# pipeline: Creamos un generador de texto a partir del modelo y tokenizer\n",
        "# HuggingFacePipeline: Lo envolvemos para que LangChain pueda usarlo\n",
        "# ConversationBufferMemory: Guarda historial de la conversaci√≥n\n",
        "# ConversationChain: Maneja la interacci√≥n entre usuario ‚Üî LLM\n",
        "#####\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# 1. Cargamos modelo desde Hugging Face\n",
        "model_name = \"tiiuae/falcon-7b-instruct\"  # Puedes usar otro m√°s ligero como \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "# 2. Creamos un pipeline de generaci√≥n\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# 3. Integramos con LangChain\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# 4. Agregamos memoria de conversaci√≥n\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# 5. Creamos la cadena conversacional\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# 6. Funci√≥n p√∫blica para responder desde la API\n",
        "def get_chat_response(message: str) -> str:\n",
        "    return conversation.run(message)"
      ],
      "metadata": {
        "id": "gv7751WH_g1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py : API con FastAPI\n",
        "\n",
        "#####\n",
        "# FastAPI: Framework para levantar el servidor web\n",
        "# BaseModel: Define los datos que esperamos recibir (solo un string)\n",
        "# /chat/: Endpoint POST que recibe un mensaje y devuelve la respuesta del modelo\n",
        "#####\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from chatbot import get_chat_response\n",
        "\n",
        "app = FastAPI(title=\"Chatbot con LLMs Open Source\")\n",
        "\n",
        "# 1. Definimos el formato del request\n",
        "class UserMessage(BaseModel):\n",
        "    message: str\n",
        "\n",
        "# 2. Endpoint de prueba\n",
        "@app.get(\"/\")\n",
        "def home():\n",
        "    return {\"message\": \"API de chatbot corriendo correctamente!\"}\n",
        "\n",
        "# 3. Endpoint principal del chatbot\n",
        "@app.post(\"/chat/\")\n",
        "def chat(user_input: UserMessage):\n",
        "    reply = get_chat_response(user_input.message)\n",
        "    return {\"response\": reply}"
      ],
      "metadata": {
        "id": "xrSo9Us2_-Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutamos\n",
        "# Instala dependencias\n",
        "pip install -r requirements.txt\n",
        "\n",
        "# Corre el servidor\n",
        "uvicorn main:app --reload"
      ],
      "metadata": {
        "id": "8OM66vuBADW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos el chatbot: Puedes usar curl, Postman o un frontend tipo Streamlit para hablar con √©l\n",
        "\n",
        "curl -X POST \"http://127.0.0.1:8000/chat/\" -H \"Content-Type: application/json\" -d '{\"message\":\"Hola, ¬øqu√© puedes hacer?\"}'"
      ],
      "metadata": {
        "id": "4l1wwA6yAIOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resultado: Hemos creado un chatbot que:\n",
        "\n",
        "Usa un modelo Hugging Face (¬°sin necesidad de OpenAI!)\\\n",
        "Guarda contexto de la conversaci√≥n\\\n",
        "Es servible como API\\\n",
        "Es f√°cilmente integrable con apps web"
      ],
      "metadata": {
        "id": "XT29oKvt_95B"
      }
    }
  ]
}