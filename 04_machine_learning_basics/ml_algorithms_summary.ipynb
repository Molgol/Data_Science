{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6OFyIaq-9dqa",
        "ErCl7DGi9m5t"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Machine Learning Algorithms Summary\n",
        "\n",
        "Este notebook resume los principales algoritmos de *machine learning supervisado*, tanto para clasificaci√≥n como para regresi√≥n, aunque nos centraremos m√°s en clasificaci√≥n.\n",
        "\n",
        "Usaremos el dataset del Titanic como ejemplo pr√°ctico para comparar:\n",
        "\n",
        "‚úÖ C√≥mo funcionan  \n",
        "‚úÖ Cu√°ndo usarlos  \n",
        "‚úÖ Sus ventajas y desventajas  \n",
        "‚úÖ Qu√© hiperpar√°metros son importantes  \n",
        "‚úÖ Qu√© tan bien predicen si alguien sobrevivi√≥\n",
        "\n"
      ],
      "metadata": {
        "id": "eP1Si_ZI6vIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparaci√≥n del dataset"
      ],
      "metadata": {
        "id": "vgGfjxIs62Sy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBXRjmUi4k0J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Cargar y preparar el dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Preprocesamiento similar al notebook anterior\n",
        "df = df.drop(columns=['deck', 'embark_town', 'alive'])\n",
        "df['age'] = df['age'].fillna(df['age'].median())\n",
        "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df = pd.get_dummies(df, columns=['embarked'], drop_first=True)\n",
        "\n",
        "# Crear nuevas variables\n",
        "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
        "df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
        "\n",
        "# Escalar\n",
        "scaler = StandardScaler()\n",
        "df[['age', 'fare']] = scaler.fit_transform(df[['age', 'fare']])\n",
        "\n",
        "# Selecci√≥n de variables\n",
        "features = ['pclass', 'sex', 'age', 'fare', 'family_size', 'is_alone', 'embarked_Q', 'embarked_S']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Divisi√≥n de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos de clasificaci√≥n\n",
        "\n",
        "Vamos a probar:\n",
        "- Logistic Regression\n",
        "- K-Nearest Neighbors (KNN)\n",
        "- Decision Trees\n",
        "- Random Forest\n",
        "- Support Vector Machines (SVM)\n",
        "- Gradient Boosting (opcional)"
      ],
      "metadata": {
        "id": "GJ_krs1v7Ajn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Logistic Regression**"
      ],
      "metadata": {
        "id": "88H_iWD87IY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9BWaz6to7ChE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcula una combinaci√≥n lineal de las variables para estimar la probabilidad de una clase (por ejemplo, sobrevivir = 1). La regresi√≥n log√≠stica te dice cu√°nto aumenta o disminuye la probabilidad de que ocurra algo al cambiar una variable.\n",
        "\n",
        "**¬øCu√°ndo usarla?**  \n",
        "- Cuando las variables tienen relaci√≥n lineal con la probabilidad de clase.\n",
        "- Es r√°pida, interpretable y sirve de baseline.\n",
        "\n",
        "‚ö†Ô∏è **Limitaciones**: No modela relaciones no lineales.\n",
        "\n",
        "Coeficientes importantes: Las que tengan coeficientes (model.coef_) m√°s alejados de 0. Por ejemplo, sex, pclass, fare.\n"
      ],
      "metadata": {
        "id": "kib1kEY37PMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. K-Nearest Neighbors (KNN)**"
      ],
      "metadata": {
        "id": "9mpx3BlE7UwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "O-9UpJKl7PpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mide la \"distancia\" entre puntos para clasificarlos (por ejemplo, si tus 5 vecinos m√°s cercanos sobrevivieron, t√∫ tambi√©n probablemente). No es interpretable directamente, pero funciona bien cuando los grupos est√°n claramente separados en el espacio.\n",
        "\n",
        "**¬øCu√°ndo usarlo?**  \n",
        "- Cuando no hay mucha data y el espacio de caracter√≠sticas tiene significado geom√©trico.\n",
        "- No necesita entrenamiento (lazy learner), solo compara con vecinos.\n",
        "\n",
        "‚ö†Ô∏è **Limitaciones**: Sensible a escala y a ruido, lento con grandes datasets.\n",
        "\n",
        "Hiperpar√°metro: `n_neighbors`, `metric`. No hay pesos directos, pero las que afecten m√°s la distancia son las que m√°s influyen. Por eso se deben escalar los datos.\n"
      ],
      "metadata": {
        "id": "w6riUkNU7huN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. √Årboles de decisi√≥n**"
      ],
      "metadata": {
        "id": "L_dh6O0C8_xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "f53hJUAL7ZAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividen el dataset en ramas seg√∫n condiciones en las variables. Son muy explicativos: puedes ver qu√© reglas tom√≥ el √°rbol para llegar a una predicci√≥n.\n",
        "\n",
        "**¬øCu√°ndo usarlo?**  \n",
        "- Si quieres interpretabilidad y reglas claras.\n",
        "- Puede capturar relaciones no lineales f√°cilmente.\n",
        "\n",
        "‚ö†Ô∏è **Limitaciones**: Overfitting si no se poda (`max_depth`, `min_samples_leaf`)\n",
        "\n",
        "Variables importantes: Las que m√°s se usan para dividir los datos y reducir incertidumbre (model.feature_importances_).\n"
      ],
      "metadata": {
        "id": "7m4oeavp9D9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Random Forest**"
      ],
      "metadata": {
        "id": "GhdQMXBu9GDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "iOy3Z-Vn9EQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combina el resultado de varios √°rboles de decisi√≥n para obtener una soluci√≥n.\n",
        "\n",
        "**¬øCu√°ndo usarlo?**  \n",
        "- Cuando quieres precisi√≥n y evitar overfitting.\n",
        "- Muy bueno para variables mixtas (num√©ricas + categ√≥ricas).\n",
        "\n",
        "‚ö†Ô∏è **Limitaciones**: Menos interpretable, m√°s pesado.\n",
        "\n",
        "Hiperpar√°metros: `n_estimators`, `max_depth`\n"
      ],
      "metadata": {
        "id": "jeJwYBUh9IOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E. SVM (Support Vector Machine)**"
      ],
      "metadata": {
        "id": "gCeELgeO9KOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel='rbf', C=1, gamma='scale')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "cp9rQ3tt9O8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encuentra un hiperplano (una frontera) que separa clases lo mejor posible. M√°s complejo de interpretar, pero √∫til para datos donde las clases no se separan f√°cilmente.\n",
        "\n",
        "**¬øCu√°ndo usarla?**  \n",
        "- Muy buena en datasets con muchas dimensiones y m√°rgenes claros.\n",
        "- Potente con pocas observaciones y datos escalados.\n",
        "\n",
        "‚ö†Ô∏è **Limitaciones**: Costosa en grandes datasets, menos interpretable.\n",
        "\n",
        "Hiperpar√°metros: `C`, `kernel`, `gamma`. Es necesario definir las variables que determinan la posici√≥n del margen del hp."
      ],
      "metadata": {
        "id": "8y-WmKts9RAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F. Gradient Boosting**"
      ],
      "metadata": {
        "id": "IW1JjxDH9SQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "OzVqUEfQ9T__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crea √°rboles secuenciales, donde cada nuevo √°rbol corrige errores del anterior.Es potente y flexible, pero no tan f√°cil de explicar sin herramientas espec√≠ficas.\n",
        "\n",
        "**¬øCu√°ndo usarlo?**  \n",
        "- Cuando quieres el mejor rendimiento posible (competencias Kaggle).\n",
        "- Captura relaciones complejas, robusto a ruido.\n",
        "\n",
        "‚ö†Ô∏è **Limitaciones**: Requiere tuning de muchos hiperpar√°metros, m√°s lento de entrenar.\n",
        "\n",
        "Variables importantes: De nuevo `.feature_importances_`. Tambi√©n puedes usar t√©cnicas como SHAP para explicabilidad m√°s profunda."
      ],
      "metadata": {
        "id": "mNakwLjX9WYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparativa de resultados"
      ],
      "metadata": {
        "id": "6OFyIaq-9dqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    print(f\"{name}: {acc:.3f}\")"
      ],
      "metadata": {
        "id": "0ck_28x_9kXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones"
      ],
      "metadata": {
        "id": "ErCl7DGi9m5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo               | Pros                           | Contras                          |\n",
        "|----------------------|--------------------------------|----------------------------------|\n",
        "| Logistic Regression  | Simple, interpretable          | No capta no-linealidad          |\n",
        "| KNN                  | F√°cil, no entrena              | Lento, sensible a escala         |\n",
        "| √Årbol de decisi√≥n    | Interpretables, no-linealidad  | Overfitting sin poda             |\n",
        "| Random Forest        | Robusto, preciso                | Menos interpretable              |\n",
        "| SVM                  | Preciso en alta dimensi√≥n       | Lento, menos interpretable       |\n",
        "| Gradient Boosting    | Muy preciso                     | M√°s costoso, tuning complejo     |\n",
        "\n",
        "El mejor modelo depende del problema, pero entender c√≥mo funciona cada uno es el primer paso hacia una buena soluci√≥n. üéØ\n"
      ],
      "metadata": {
        "id": "-0Df-tP39o7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##¬†(ANEXO) GLM: Generalized Linear Models\n",
        "Los GLM (Modelos Lineales Generalizados) son una generalizaci√≥n de la regresi√≥n lineal/log√≠stica que permiten otros tipos de distribuci√≥n de error y funciones de enlace."
      ],
      "metadata": {
        "id": "38mVarwjDYQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "X_glm = sm.add_constant(X_train)  # se agrega constante (intercepto)\n",
        "glm_model = sm.GLM(y_train, X_glm, family=sm.families.Binomial())\n",
        "glm_results = glm_model.fit()\n",
        "\n",
        "print(glm_results.summary())"
      ],
      "metadata": {
        "id": "pl3v67toDdqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resumen contiene:\n",
        "* Una tabla detallada con los coeficientes de cada variable\n",
        "* Valores p para evaluar la significancia estad√≠stica\n",
        "* Odds ratio interpretables\n",
        "* R^2 pseudo para calidad del ajuste\n",
        "\n",
        "\n",
        "**¬øCu√°ndo usarla?**  \n",
        "- Cuando necesitas interpretabilidad estad√≠stica fuerte\n",
        "- Cuando el proyecto es m√°s cient√≠fico o acad√©mico\n",
        "- Cuando quieres probar hip√≥tesis sobre variables espec√≠ficas\n",
        "\n",
        "> Otros tipos de familias son: Poisson (para contar eventos (ej. n√∫mero de compras)) o Gamma (para valores positivos continuos (costes, tiempos))"
      ],
      "metadata": {
        "id": "DmxdwuxgviNI"
      }
    }
  ]
}