{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6OFyIaq-9dqa",
        "ErCl7DGi9m5t"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Machine Learning Algorithms Summary\n",
        "\n",
        "This notebook summarizes the main *supervised machine learning* algorithms, focusing on **classification** (though **regression** is also mentioned).\n",
        "\n",
        "We‚Äôll use the Titanic dataset as a practical example to compare:\n",
        "\n",
        "‚úÖ How they work\n",
        "\n",
        "‚úÖ When to use them\n",
        "\n",
        "‚úÖ Their advantages and disadvantages\n",
        "\n",
        "‚úÖ Which hyperparameters are important\n",
        "\n",
        "‚úÖ How well they predict survival\n"
      ],
      "metadata": {
        "id": "eP1Si_ZI6vIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation"
      ],
      "metadata": {
        "id": "vgGfjxIs62Sy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZBXRjmUi4k0J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Preprocessing similar to previous notebooks: dropping irrelevant columns, imputing missing values and encoding categorical variables\n",
        "df = df.drop(columns=['deck', 'embark_town', 'alive'])\n",
        "df['age'] = df['age'].fillna(df['age'].median())\n",
        "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df = pd.get_dummies(df, columns=['embarked'], drop_first=True)\n",
        "\n",
        "# Create additional features\n",
        "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
        "df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
        "\n",
        "# Scale numerical variables (mean 0, variance 1)\n",
        "scaler = StandardScaler()\n",
        "df[['age', 'fare']] = scaler.fit_transform(df[['age', 'fare']])\n",
        "\n",
        "# Select the most relevant variables\n",
        "features = ['pclass', 'sex', 'age', 'fare', 'family_size', 'is_alone', 'embarked_Q', 'embarked_S']\n",
        "X = df[features]\n",
        "y = df['survived'] # The goal of the model is to predict whether a given person survives or not\n",
        "\n",
        "# Split the data into training and testing sets (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification models\n",
        "\n",
        "Since the target variable is 1 or 0, we'll start by applying classification models. We will test the following models:\n",
        "\n",
        "- Logistic Regression\n",
        "- K-Nearest Neighbors (KNN)\n",
        "- Decision Trees\n",
        "- Random Forest\n",
        "- Support Vector Machines (SVM)\n",
        "- Gradient Boosting"
      ],
      "metadata": {
        "id": "GJ_krs1v7Ajn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A. Logistic Regression**"
      ],
      "metadata": {
        "id": "88H_iWD87IY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train the model and make predictions\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "### Generate a report with various classification metrics\n",
        "'''\n",
        "Precision: Of the ones I predicted as positive, how many actually were?\n",
        "Recall: Of the actual positives, how many did I correctly predict?\n",
        "F1-score: Arithmetic mean between precision and recall\n",
        "Support: Number of actual samples in each class (from y_test)\n",
        "\n",
        "Accuracy: % of total correct predictions\n",
        "Macro/Weighted avg: simple/weighted averages across classes\n",
        "'''\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9BWaz6to7ChE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3bf375-1214-4f36-a3d0-9d662f30c54a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       105\n",
            "           1       0.78      0.72      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression computes a linear combination of features to estimate the probability of a class (e.g., survival = 1). It tells you how much the probability of an event increases or decreases as a feature changes.\n",
        "\n",
        "**When to use it?**\n",
        "- When features have a linear relationship with the class probability.\n",
        "- Fast, interpretable, and useful as a baseline.\n",
        "\n",
        "‚ö†Ô∏è **Limitations:** Cannot model non-linear relationships.\n",
        "\n",
        "Important coefficients: features with coefficients (`model.coef_`) farthest from 0 (e.g., sex, pclass, fare)"
      ],
      "metadata": {
        "id": "kib1kEY37PMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B. K-Nearest Neighbors (KNN)**"
      ],
      "metadata": {
        "id": "9mpx3BlE7UwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "O-9UpJKl7PpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286e1db4-49b1-4ba3-dc42-8d86d4ed04cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86       105\n",
            "           1       0.82      0.76      0.79        74\n",
            "\n",
            "    accuracy                           0.83       179\n",
            "   macro avg       0.83      0.82      0.82       179\n",
            "weighted avg       0.83      0.83      0.83       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It measures the \"distance\" between points to classify them (for example, if your 5 nearest neighbors survived, you probably did too). It is not directly interpretable but works well when groups are clearly separated in feature space.\n",
        "\n",
        "**When to use it?**\n",
        "- When you have little data and the feature space has geometric meaning.\n",
        "- No training required (lazy learner), just comparison with neighbors.\n",
        "\n",
        "‚ö†Ô∏è **Limitations:** Sensitive to feature scaling and noise. Slow with large datasets.\n",
        "\n",
        "Hyperparameters: `n_neighbors`, `wheights`,`metric`. No direct weights, but the variables that most affect the distance will have the most influence (that's why scaling the data is important)."
      ],
      "metadata": {
        "id": "w6riUkNU7huN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C. Decision trees**"
      ],
      "metadata": {
        "id": "L_dh6O0C8_xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "f53hJUAL7ZAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7eadb89-971e-4ede-9ff2-1303dad761ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.91      0.84       105\n",
            "           1       0.84      0.64      0.72        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.81      0.77      0.78       179\n",
            "weighted avg       0.80      0.80      0.79       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees split the data based on feature conditions. They are highly interpretable, allowing you to see the rules used for each prediction.\n",
        "\n",
        "**When to use them?**\n",
        "\n",
        "- When you need interpretability and clear decision rules.\n",
        "- Can easily capture non-linear relationships.\n",
        "\n",
        "‚ö†Ô∏è **Limitations:** Prone to overfitting if not pruned (`max_depth`, `min_samples_leaf`).\n",
        "\n",
        "Important features:\n",
        "Those most used for splitting and reducing uncertainty (`model.feature_importances`).\n"
      ],
      "metadata": {
        "id": "7m4oeavp9D9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To see the importance of the variables we can use 'feature_importances_'\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(feature_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vusYhRZ_N4XB",
        "outputId": "71889744-25e2-4d1a-c321-4b8de6fa0525"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       feature  importance\n",
            "1          sex    0.579608\n",
            "0       pclass    0.200498\n",
            "3         fare    0.081064\n",
            "2          age    0.076410\n",
            "4  family_size    0.048520\n",
            "7   embarked_S    0.013900\n",
            "5     is_alone    0.000000\n",
            "6   embarked_Q    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D. Random Forest**"
      ],
      "metadata": {
        "id": "GhdQMXBu9GDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "iOy3Z-Vn9EQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f52280-b679-439e-9332-d5a56b55bd91"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.87      0.86       105\n",
            "           1       0.81      0.80      0.80        74\n",
            "\n",
            "    accuracy                           0.84       179\n",
            "   macro avg       0.83      0.83      0.83       179\n",
            "weighted avg       0.84      0.84      0.84       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest combines the results of multiple decision trees to improve accuracy and reduce overfitting.\n",
        "\n",
        "**When to use it?**\n",
        "- When you want accuracy and robustness against overfitting.\n",
        "- Very effective with mixed variables (numerical + categorical).\n",
        "\n",
        "‚ö†Ô∏è **Limitations:** Less interpretable than single trees. More resource-intensive or slower.\n",
        "\n",
        "Hyperparameters: `n_estimators`, `max_depth`\n"
      ],
      "metadata": {
        "id": "jeJwYBUh9IOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E. SVM (Support Vector Machine)**"
      ],
      "metadata": {
        "id": "gCeELgeO9KOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel='rbf', C=1, gamma='scale')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "cp9rQ3tt9O8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c795e018-a1af-4ecf-d7b1-a74e176f56f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84       105\n",
            "           1       0.80      0.72      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM finds a hyperplane (a decision boundary) that best separates the classes. It's more complex to interpret but useful when the classes aren't easily separable.\n",
        "\n",
        "**When to use it?**\n",
        "- Good for high-dimensional datasets with clear margins.\n",
        "- Powerful with few observations and scaled data.\n",
        "\n",
        "‚ö†Ô∏è **Limitations:** Computationally expensive on large datasets. Less interpretable\n",
        "\n",
        "Hyperparameters: `C`, `kernel`, `gamma`. It's necessary to define the variables that determine the position of the hyperplane's margin."
      ],
      "metadata": {
        "id": "8y-WmKts9RAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F. Gradient Boosting**"
      ],
      "metadata": {
        "id": "IW1JjxDH9SQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "OzVqUEfQ9T__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993ed489-5e8b-4588-d7a9-e7682ed16129"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.89      0.84       105\n",
            "           1       0.81      0.69      0.74        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.81      0.79      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It builds sequential trees, where each new tree corrects the errors of the previous one. It is powerful and flexible but harder to interpret without special tools.\n",
        "\n",
        "**When to use it?**\n",
        "- When you want the best possible performance\n",
        "- Captures complex relationships and is robust to noise\n",
        "\n",
        "‚ö†Ô∏è **Limitations:** Requires tuning of many hyperparameters, slower to train\n",
        "\n",
        "Important variables: Again, with `.feature_importances_`. You can also use techniques like SHAP for deeper explainability."
      ],
      "metadata": {
        "id": "mNakwLjX9WYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model comparison"
      ],
      "metadata": {
        "id": "6OFyIaq-9dqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Compare the accuracy of each model used\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    print(f\"{name}: {acc:.3f}\")"
      ],
      "metadata": {
        "id": "0ck_28x_9kXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6cfaf8-5da0-48a6-e709-81daea134da5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression: 0.799\n",
            "KNN: 0.832\n",
            "Decision Tree: 0.788\n",
            "Random Forest: 0.838\n",
            "SVM: 0.810\n",
            "Gradient Boosting: 0.804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-validation** splits the data into several folds: trains the model on some folds, and tests it on the remaining ones. This process is repeated so every observation is used for both training and testing, reducing variance in performance estimates.\n",
        "\n",
        "Why is it important?\n",
        "- To have a more robust measure of the model's performance (better than a single train-test split).\n",
        "- To avoid overfitting or underfitting by evaluating the model on multiple subsets."
      ],
      "metadata": {
        "id": "m1x-JdneIicV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cross-validated accuracy scores (5-fold):\\n\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Compute cross-validated accuracy (5 folds by default)\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "    print(f\"{name}: Mean={scores.mean():.3f}, Std={scores.std():.3f}\")"
      ],
      "metadata": {
        "id": "zICCVN40IfvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca233dc-cf6d-4f46-89ee-1a57fb563474"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy scores (5-fold):\n",
            "\n",
            "Logistic Regression: Mean=0.794, Std=0.017\n",
            "KNN: Mean=0.800, Std=0.019\n",
            "Decision Tree: Mean=0.787, Std=0.023\n",
            "Random Forest: Mean=0.802, Std=0.026\n",
            "SVM: Mean=0.825, Std=0.019\n",
            "Gradient Boosting: Mean=0.833, Std=0.025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions"
      ],
      "metadata": {
        "id": "ErCl7DGi9m5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model               | Pros                           | Cons                          |\n",
        "|----------------------|--------------------------------|----------------------------------|\n",
        "| Logistic Regression  | Simple, interpretable          | Cannot capture non-linearity          |\n",
        "| KNN                  | Easy, no training required              | Slow, sensitive to scaling         |\n",
        "| Decision trees    | Interpretable, captures non-linearity  | Overfitting without pruning             |\n",
        "| Random Forest        | Robust, accurate                | Less interpretable              |\n",
        "| SVM                  | Accurate in high-dimensional spaces       | Slow, less interpretable       |\n",
        "| Gradient Boosting    | Very accurate                     | More expensive, complex tuning     |\n",
        "\n",
        "The best model depends on the problem, but understanding how each works is the first step toward a good solution. üéØ\n"
      ],
      "metadata": {
        "id": "-0Df-tP39o7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression model"
      ],
      "metadata": {
        "id": "a83PZ0zRQFs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can work with the same dataset by changing the target variable. To do so, the goal is not a class but a continuous variable, such as the `fare` field."
      ],
      "metadata": {
        "id": "GlbzRISt7iNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the new target variable\n",
        "y_reg = df['fare']  # Change to a continuous target\n",
        "X_reg = df[['pclass', 'sex', 'age', 'family_size', 'is_alone', 'embarked_Q', 'embarked_S']]\n",
        "\n",
        "# Split the data in the same way\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear regression model (one of the simplest)\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_reg = reg_model.predict(X_test_reg)\n",
        "\n",
        "# Evaluate using regression metrics (MSE, MAE,R2..)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R¬≤ Score: {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArInNAkRRIjU",
        "outputId": "cff29bef-72f9-4707-db03-ed0fd40e2698"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.37\n",
            "R¬≤ Score: 0.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the regression model is not good enough: the mean squared error is too high, and the explainability of the target variable (R¬≤) is very low.\n",
        "\n",
        "Therefore, we will explore two more models to test two hypotheses:\n",
        "- A: The model is too simple and fails to capture the underlying relationships\n",
        "- B: We don‚Äôt have enough features or data to model the target variable effectively"
      ],
      "metadata": {
        "id": "p_WoTrPmTPRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Select a different set of variables.\n",
        "y_reg = df['fare']  # Target variable\n",
        "X_reg = df[['pclass', 'sex', 'age', 'family_size', 'parch', 'embarked_Q', 'embarked_S']]\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict\n",
        "y_pred_reg = rf_regressor.predict(X_test_reg)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"Random Forest Regressor MSE: {mse:.2f}\")\n",
        "print(f\"Random Forest Regressor R¬≤: {r2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-2Gk1yqURKr",
        "outputId": "d6485749-6bc1-4a8c-d377-bc80cbc1ddad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor MSE: 0.68\n",
            "Random Forest Regressor R¬≤: -0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_regressor.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predecimos\n",
        "y_pred_gb = gb_regressor.predict(X_test_reg)\n",
        "\n",
        "# Y evaluamos\n",
        "mse_gb = mean_squared_error(y_test_reg, y_pred_gb)\n",
        "r2_gb = r2_score(y_test_reg, y_pred_gb)\n",
        "\n",
        "print(f\"Gradient Boosting Regressor MSE: {mse_gb:.2f}\")\n",
        "print(f\"Gradient Boosting Regressor R¬≤: {r2_gb:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-lu5cdvU8Lh",
        "outputId": "45098438-3b16-4489-d40f-bd6c399951b3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor MSE: 0.53\n",
            "Gradient Boosting Regressor R¬≤: 0.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In both cases, we get a model that performs worse than the first one... (even with RandomForest, we get an R¬≤ score below 0, which indicates a lack of meaningful explanation).\n",
        "\n",
        "Thus, we can conclude that with the current variables, we **cannot build a useful regression model**. In the future, we could try:\n",
        "\n",
        "- Reviewing which features are more strongly related to the target.\n",
        "- Doing feature engineering (creating new variables, normalizing, or scaling existing ones).\n",
        "- Trying more robust models.\n",
        "- Expanding the dataset."
      ],
      "metadata": {
        "id": "hiLUogl_VEBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##¬†(APPENDIX A) GLM: Generalized Linear Models\n",
        "GLMs (Generalized Linear Models) are a flexible extension of linear/logistic regression that allow modeling different types of response variables using appropriate distributions and link functions."
      ],
      "metadata": {
        "id": "38mVarwjDYQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# First remove the categorical variables\n",
        "X_train = X_train.drop(columns=['embarked_Q', 'embarked_S'])\n",
        "\n",
        "##¬†Add a constant term\n",
        "# In linear and GLM models, the intercept (or constant term) represents the baseline prediction when all variables are zero\n",
        "X_glm = sm.add_constant(X_train)\n",
        "\n",
        "# Use the Binomial family, as this is equivalent to performing logistic regression (the goal is to predict a binary variable)\n",
        "glm_model = sm.GLM(y_train, X_glm, family=sm.families.Binomial())\n",
        "glm_results = glm_model.fit()\n",
        "\n",
        "print(glm_results.summary())"
      ],
      "metadata": {
        "id": "pl3v67toDdqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ff8b05-db6c-48d7-fe3d-43a0b6949024"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Generalized Linear Model Regression Results                  \n",
            "==============================================================================\n",
            "Dep. Variable:               survived   No. Observations:                  712\n",
            "Model:                            GLM   Df Residuals:                      705\n",
            "Model Family:                Binomial   Df Model:                            6\n",
            "Link Function:                  Logit   Scale:                          1.0000\n",
            "Method:                          IRLS   Log-Likelihood:                -317.93\n",
            "Date:                Wed, 11 Jun 2025   Deviance:                       635.86\n",
            "Time:                        06:37:34   Pearson chi2:                     711.\n",
            "No. Iterations:                     5   Pseudo R-squ. (CS):             0.3505\n",
            "Covariance Type:            nonrobust                                         \n",
            "===============================================================================\n",
            "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------\n",
            "const           1.7466      0.456      3.833      0.000       0.854       2.640\n",
            "pclass         -0.9082      0.156     -5.835      0.000      -1.213      -0.603\n",
            "sex             2.6971      0.220     12.282      0.000       2.267       3.128\n",
            "age            -0.4046      0.113     -3.574      0.000      -0.626      -0.183\n",
            "fare            0.1479      0.122      1.209      0.227      -0.092       0.388\n",
            "family_size    -0.4171      0.108     -3.868      0.000      -0.628      -0.206\n",
            "is_alone       -0.7367      0.300     -2.455      0.014      -1.325      -0.149\n",
            "===============================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summary includes:\n",
        "* Coefficients ‚Üí how much each variable influences the outcome\n",
        "* Standard error ‚Üí precision of the coefficients\n",
        "* z-value ‚Üí coefficient divided by its standard error (for hypothesis testing)\n",
        "* P-value ‚Üí whether the coefficient is significantly different from 0\n",
        "* Confidence intervals (95% CI) ‚Üí range where the true value is likely to fall\n",
        "* Deviance, Pearson Chi2 ‚Üí model fit metrics\n",
        "* Pseudo R¬≤ ‚Üí a measure of how well the model fits (similar to regression R¬≤)\n",
        "\n",
        "**When to use it?**\n",
        "- When strong statistical interpretability is needed\n",
        "- When the project is scientific or academic\n",
        "- When testing hypotheses about specific variables\n",
        "\n",
        "Other types of families include: Poisson (for counting events, e.g., number of purchases) or Gamma (for positive continuous values like costs or durations)"
      ],
      "metadata": {
        "id": "DmxdwuxgviNI"
      }
    }
  ]
}